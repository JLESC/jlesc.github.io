<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Joint Laboratory on Extreme Scale Computing</title>
    <description>The Joint Laboratory for Extreme Scale Computing includes researchers from the French National Institute for Research in Computer Science and Control (INRIA), the University of Illinois at Urbana-Champaign’s Center for Extreme-Scale Computation, the National Center for Supercomputing Applications, Argonne National Laboratory, Barcelona Supercomputing Center, Julich Supercomputing Center and Riken/AICS. The Joint Lab is part of Parallel@Illinois.
</description>
    <link>https://jlesc.github.io/</link>
    <atom:link href="https://jlesc.github.io/feed.xml" rel="self" type="application/rss+xml" />
    
      <item>
        <title>15th JLESC Workshop</title>
        <description>&lt;p&gt;The 15th JLESC Workshop will take place at INRIA Bordeaux, from March 21 to 23, 2023.&lt;/p&gt;

&lt;!--more--&gt;

&lt;p&gt;Click &lt;a href=&quot;/events/15th-jlesc-workshop&quot;&gt;here&lt;/a&gt; for more information!&lt;/p&gt;
</description>
        <pubDate>Mon, 17 Oct 2022 00:00:00 +0000</pubDate>
        <link>https://jlesc.github.io/news/2022/10/17/15th-workshop/</link>
        <guid isPermaLink="true">https://jlesc.github.io/news/2022/10/17/15th-workshop/</guid>
      </item>
    
      <item>
        <title>14th JLESC Workshop</title>
        <description>&lt;p&gt;The 14th JLESC Workshop will take place at The University of Illinois at Urbana-Champaign, from September 28 to 30.&lt;/p&gt;

&lt;!--more--&gt;

&lt;p&gt;Click &lt;a href=&quot;/events/14th-jlesc-workshop&quot;&gt;here&lt;/a&gt; for more information!&lt;/p&gt;
</description>
        <pubDate>Thu, 16 Sep 2021 00:00:00 +0000</pubDate>
        <link>https://jlesc.github.io/news/2021/09/16/14th-workshop/</link>
        <guid isPermaLink="true">https://jlesc.github.io/news/2021/09/16/14th-workshop/</guid>
      </item>
    
      <item>
        <title>13th JLESC Workshop</title>
        <description>&lt;p&gt;The 13th JLESC Workshop goes virtual, Dec 14 to 16, 2021.&lt;/p&gt;

&lt;!--more--&gt;

&lt;p&gt;Click &lt;a href=&quot;/events/13th-jlesc-workshop&quot;&gt;here&lt;/a&gt; for more information!&lt;/p&gt;
</description>
        <pubDate>Wed, 01 Sep 2021 00:00:00 +0000</pubDate>
        <link>https://jlesc.github.io/news/2021/09/01/13th-workshop/</link>
        <guid isPermaLink="true">https://jlesc.github.io/news/2021/09/01/13th-workshop/</guid>
      </item>
    
      <item>
        <title>Open postdoc positions at ANL</title>
        <description>&lt;p&gt;ANL has an number of open positions for postdocs!&lt;/p&gt;

&lt;!--more--&gt;

&lt;h4 id=&quot;postdoc-datastates---tracking-versioning-and-reuse-of-intermediate-data&quot;&gt;&lt;a href=&quot;https://bit.ly/3lcKCPM&quot;&gt;Postdoc: DataStates - Tracking, Versioning and Reuse of Intermediate Data&lt;/a&gt;&lt;/h4&gt;

&lt;p&gt;Contact: &lt;a href=&quot;bnicolae@anl.gov&quot;&gt;Bogdan Nicolae&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Description: We are exploring a new data model centered around the notion of data states, which are intermediate representations of datasets automatically recorded into a lineage when tagged by applications with hints, constraints and persistency semantics. Such an approach enables the applications to focus on the meaning and properties of their data rather than how to access it, effectively reducing complexity while unlocking high performance and scalability for many use cases: finding and reusing previous intermediate results to explore alternatives, inspecting the evolution of datasets, verifying correctness, etc. This is especially important in the context of deep learning, where there is an acute need for advanced tools that explore many alternative DNN models and/or ensembles to improve accuracy, training speed and ability to generalize/explain a problem.&lt;/p&gt;

&lt;p&gt;Link: &lt;a href=&quot;https://bit.ly/3lcKCPM&quot;&gt;https://bit.ly/3lcKCPM&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;postdocphd-subcontractinternship-rrr-robustness-reconfigurability-reproducibility-for-hpcbdai-workflows&quot;&gt;Postdoc/PhD subcontract/internship: RRR (robustness, reconfigurability, reproducibility) for HPC+BD+AI workflows&lt;/h4&gt;

&lt;p&gt;Contact: &lt;a href=&quot;bnicolae@anl.gov&quot;&gt;Bogdan Nicolae&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Description: We seek three properties (abbreviated RRR) for hybrid workflows composed of HPC, BD, and AI: robustness (gracefully handle failures and other unexpected events);  reconfigurability (dynamically adapt to changing conditions during run-time for optimal performance and scalability); reproducibility (able to replay part or all of a workflow under similar conditions in order to verify the results). In this context, we are exploring novel techniques that exploit relaxed inter-task dependencies to optimize checkpoint-restart strategies (e.g. no need to roll back if enough survivors are in a consistent state, workflow orchestration based on up-to-date overview of behavioral patterns, dynamic reproducibility based on capturing decision provenance.&lt;/p&gt;

&lt;h4 id=&quot;postdocphd-subcontractinternship-deep-learning-oriented-streams-with-revisit-support&quot;&gt;Postdoc/PhD subcontract/internship: Deep Learning-oriented streams with revisit support&lt;/h4&gt;

&lt;p&gt;Description: Modern deep learning is not static: training sets: new training data is constantly arriving. However, DNN models cannot be simply trained incrementally due to the problem of catastrophic forgetting, i.e., bias in favor of newer samples at the expense of older ones. Therefore, old samples need to be persisted and revisited, which is not a pattern the storage solutions used by state-of-the-art approaches (e.g., parallel file systems) are optimized to address. In this context, one idea is to extract representative old samples and combine them with new samples such as to enable unbiased retraining. This internship will explore techniques to achieve this goal while mitigating the I/O performance and scalability issues associated with the constant accumulation of new samples (which slows down I/O performance over time).&lt;/p&gt;

&lt;h3 id=&quot;postdoc-in-the-context-of-the-ecp-project-ai-based-prediction-for-data-reduction-and-interference-avoidance&quot;&gt;Postdoc in the context of the ECP project: AI based prediction for data reduction and interference avoidance.&lt;/h3&gt;

&lt;p&gt;Contact: &lt;a href=&quot;cappello@anl.gov&quot;&gt;Franck Cappello&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Description: The combination of AI and HPC offers new opportunity to improve the performance of scientific applications. How AI can help accelerate HPC operations and ultimately scientific application executions is an important question in this context. The postdoc position will explore how to leverage AI to accelerate data transformations and management in Exascale systems. We list here only 2 examples of potential directions. Other research directions in this context are possible depending on candidate interests and skills. One first potential direction is how to leverage AI to improve data reduction. Recent attempts of using auto-encoders for lossy data reduction are showing promising results. More exploration is needed to better understand and control AI model data reduction performance (ratio, speed, accuracy), potentially in combination with other data reduction techniques. Another potential direction is to explore optimization of asynchronous data movement scheduling between resources of exascale systems. A promising direction is AI based interference avoidance that has already shown positive results on relatively simple cases. How AI based interference avoidance perform on more complex cases and how to optimize it remain open questions.&lt;/p&gt;

&lt;h3 id=&quot;postdoctoral-appointee--computer-science--tracing-heterogeneous-apis-for-exascale&quot;&gt;&lt;a href=&quot;https://argonne.wd1.myworkdayjobs.com/en-US/Argonne_Careers/job/Argonne-National-Laboratory/Postdoctoral-Appointee---Tracing-Heterogeneous-APIs-for-Exascale_411476&quot;&gt;Postdoctoral Appointee – Computer Science – Tracing Heterogeneous APIs for Exascale&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;Contact: &lt;a href=&quot;bvideau@anl.gov&quot;&gt;Brice Videau&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Description: ALCF’s performance engineering group is looking for a post-doctoral appointee to perform research and development on a collection of tracers and their uses, in the context of the upcoming exascale platforms, and Aurora in particular. By applying techniques derived from Model Centric Debugging, the candidate will collaborate with application developers and other Argonne Computer Scientists to improve the scope and usefulness of the tracing framework for Heterogeneous computing APIs. The work will take place in a multi-disciplinary environment and will offer opportunities to interact with a wide range of talents from the whole spectrum of HPC research. The successful candidate is expected to contribute into several of the following areas: profiling accelerator usage of HPC applications; debugging accelerator usage; capturing traces that can be reinjected in simulation frameworks; extracting kernels for replay, allowing study and tuning in a sand-box; lightweight and transparent monitoring of platform usage.&lt;/p&gt;

&lt;h2 id=&quot;about-postdoc-positions-at-argonne-national-laboratory&quot;&gt;About postdoc positions at Argonne National Laboratory:&lt;/h2&gt;

&lt;p&gt;In addition to addressing such transformative challenges that arise at the intersection of HPC, big data analytics and machine learning,  postdocs have the opportunity to work closely with many domain experts to identify the requirements and bottlenecks of real-life scientific applications that address the needs of our society over the next decades. In general, you will be part of a vibrant and diverse research community from more than 100 countries. Our lab will host Aurora, one of the first Exascale supercomputers in the world, which you will have an opportunity to use for your experiments. In addition, you will have access to a large array of leading-edge experimental testbeds through the Joint Laboratory for System Evaluation (JLSE), which feature the latest technologies from top vendors like Intel, NVIDIA, AMD, CEREBRAS, etc.&lt;/p&gt;

&lt;h2 id=&quot;about-argonne-national-laboratory&quot;&gt;About Argonne National Laboratory:&lt;/h2&gt;

&lt;p&gt;As an equal employment opportunity and affirmative action employer, and in accordance with our core values of impact, safety, respect, integrity and teamwork, Argonne National Laboratory is committed to a diverse and inclusive workplace that fosters collaborative scientific discovery and innovation. In support of this commitment, Argonne encourages minorities, women, veterans and individuals with disabilities to apply for employment. Argonne considers all qualified applicants for employment without regard to age, ancestry, citizenship status, color, disability, gender, gender identity, genetic information, marital status, national origin, pregnancy, race, religion, sexual orientation, veteran status or any other characteristic protected by law. Argonne employees, and certain guest researchers and contractors, are subject to particular restrictions related to participation in Foreign Government Talent Recruitment Programs, as defined and detailed in United States Department of Energy Order 486.1. You will be asked to disclose any such participation in the application phase for review by Argonne’s Legal Department.&lt;/p&gt;
</description>
        <pubDate>Tue, 27 Apr 2021 00:00:00 +0000</pubDate>
        <link>https://jlesc.github.io/news/2021/04/27/postdoc-positions-at-anl/</link>
        <guid isPermaLink="true">https://jlesc.github.io/news/2021/04/27/postdoc-positions-at-anl/</guid>
      </item>
    
      <item>
        <title>12th JLESC Workshop</title>
        <description>&lt;p&gt;The 12th JLESC Workshop will be held online from February 24 to 26, 2021.&lt;/p&gt;

&lt;!--more--&gt;

&lt;p&gt;Click &lt;a href=&quot;/events/12th-jlesc-workshop&quot;&gt;here&lt;/a&gt; for more information!&lt;/p&gt;
</description>
        <pubDate>Fri, 11 Dec 2020 00:00:00 +0000</pubDate>
        <link>https://jlesc.github.io/news/2020/12/11/12th-workshop/</link>
        <guid isPermaLink="true">https://jlesc.github.io/news/2020/12/11/12th-workshop/</guid>
      </item>
    
      <item>
        <title>11th JLESC Workshop</title>
        <description>&lt;p&gt;The 11th JLESC Workshop, the first virtual one in the history of the JLESC, will be held online from September 8 to 10, 2020.&lt;/p&gt;

&lt;!--more--&gt;

&lt;p&gt;Click &lt;a href=&quot;/events/11th-jlesc-workshop&quot;&gt;here&lt;/a&gt; for more information!&lt;/p&gt;
</description>
        <pubDate>Mon, 10 Aug 2020 00:00:00 +0000</pubDate>
        <link>https://jlesc.github.io/news/2020/08/10/11th-workshop/</link>
        <guid isPermaLink="true">https://jlesc.github.io/news/2020/08/10/11th-workshop/</guid>
      </item>
    
      <item>
        <title>10th JLESC Workshop</title>
        <description>&lt;p&gt;The 10th JLESC Workshop has been cancelled.&lt;/p&gt;

&lt;!--more--&gt;

&lt;p&gt;Click &lt;a href=&quot;/events/10th-jlesc-workshop&quot;&gt;here&lt;/a&gt; for more information!&lt;/p&gt;
</description>
        <pubDate>Tue, 30 Apr 2019 00:00:00 +0000</pubDate>
        <link>https://jlesc.github.io/news/2019/04/30/10th-workshop/</link>
        <guid isPermaLink="true">https://jlesc.github.io/news/2019/04/30/10th-workshop/</guid>
      </item>
    
      <item>
        <title>9th JLESC Workshop</title>
        <description>&lt;p&gt;The 9th JLESC Workshop will be held at the University of Tennessee, Knoxville, USA, from April 15 to 17, 2019.&lt;/p&gt;

&lt;!--more--&gt;

&lt;p&gt;Click &lt;a href=&quot;/events/9th-jlesc-workshop&quot;&gt;here&lt;/a&gt; for more information!&lt;/p&gt;
</description>
        <pubDate>Tue, 04 Dec 2018 00:00:00 +0000</pubDate>
        <link>https://jlesc.github.io/news/2018/12/04/9th-workshop/</link>
        <guid isPermaLink="true">https://jlesc.github.io/news/2018/12/04/9th-workshop/</guid>
      </item>
    
      <item>
        <title>Training course on intra-node performance bottlenecks</title>
        <description>&lt;p&gt;JSC is welcoming registrations for the training course “&lt;a href=&quot;http://www.fz-juelich.de/SharedDocs/Termine/IAS/JSC/EN/courses/2018/intranode-performance-2018.html&quot;&gt;From zero to hero: Understanding and fixing intra-node performance bottlenecks&lt;/a&gt;”!&lt;/p&gt;

&lt;!--more--&gt;

&lt;p&gt;In the first part of the training course we want to give insights in today’s CPU microarchitecture and apply this knowledge in the hands-on session. As a demonstrator we will use a simple Coulomb solver and improve the code step-by-step. We will start from a basic implementation and advance to an optimized version using hardware features like vectorization to increase performance.&lt;/p&gt;

&lt;p&gt;The exercises will also contain training on the use of open-source tools to measure and understand the achieved performance. Such optimizations, however, depend heavily on the targeted hardware and should not be part of the algorithmic layer of the code.&lt;/p&gt;

&lt;p&gt;In the second part we will present a detailed description of possible abstraction layers to hide such hardware-specifics and therefore maintain readability and maintainability. We will also discuss the overhead costs of our introduced abstraction and show compile-time SIMD configurations and corresponding performance results on different platforms.&lt;/p&gt;

&lt;p&gt;This advanced course is free of charge and intended for scientists/developers who want to understand performance-critical hardware features of modern CPUs (like SIMD, ILP, caches, out-of-order execution) and utilize these features in their code. See &lt;a href=&quot;http://www.fz-juelich.de/SharedDocs/Termine/IAS/JSC/EN/courses/2018/intranode-performance-2018.html&quot;&gt;here&lt;/a&gt; for details.&lt;/p&gt;
</description>
        <pubDate>Tue, 06 Feb 2018 00:00:00 +0000</pubDate>
        <link>https://jlesc.github.io/news/2018/02/06/training-course-at-jsc/</link>
        <guid isPermaLink="true">https://jlesc.github.io/news/2018/02/06/training-course-at-jsc/</guid>
      </item>
    
      <item>
        <title>Open JLESC PhD position at JSC</title>
        <description>&lt;p&gt;JSC has an open position for a &lt;a href=&quot;http://www.fz-juelich.de/SharedDocs/Stellenangebote/_common/dipldok/d026-2018-jsc.html&quot;&gt;PhD student&lt;/a&gt; in the field of C++ tasking approaches!&lt;/p&gt;

&lt;!--more--&gt;

&lt;p&gt;Jülich Supercomputing Centre is recruiting a PhD student in the field
of computer science with a special focus on C++-based tasking approaches.&lt;/p&gt;

&lt;p&gt;The goal of this project is the extension and optimization of our C++
tasking framework for current and future HPC hardware as well as its
application to multigrid methods and the FMM. The project is
embedded into the interdisciplinary research setting of the Joint
Laboratory for Extreme Scale Computing (JLESC).&lt;/p&gt;

&lt;p&gt;Candidates should hold a university degree in the field of computer science or 
mathematics and should have profound understanding of C++
template metaprogramming, with no fear of numerical methods. Social
competence and good communication skills for collaborative work in and
with interdisciplinary teams are essential.&lt;/p&gt;

&lt;p&gt;The project runs for 3 years, for more information, please see &lt;a href=&quot;http://www.fz-juelich.de/SharedDocs/Stellenangebote/_common/dipldok/d026-2018-jsc.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
        <pubDate>Tue, 06 Feb 2018 00:00:00 +0000</pubDate>
        <link>https://jlesc.github.io/news/2018/02/06/phd-position-at-jsc/</link>
        <guid isPermaLink="true">https://jlesc.github.io/news/2018/02/06/phd-position-at-jsc/</guid>
      </item>
    
  </channel>
</rss>
