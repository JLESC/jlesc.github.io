<!DOCTYPE html>
<html lang="en">
  <head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>JLESC &mdash; 3rd JLESC Workshop</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="The Joint Laboratory for Extreme Scale Computing includes researchers from the French National Institute for Research in Computer Science and Control (INRIA), the University of Illinois at Urbana-Champaign’s Center for Extreme-Scale Computation, the National Center for Supercomputing Applications, Argonne National Laboratory, Barcelona Supercomputing Center, Julich Supercomputing Center and Riken/AICS. The Joint Lab is part of Parallel@Illinois.
">
  <link rel="canonical" href="https://jlesc.github.io/events/3rd-jlesc-workshop/schedule/">

  <link type="application/atom+xml" rel="alternate" href="https://jlesc.github.io/feed.xml" title="" />
  <link type="text/css" rel="stylesheet" href="/assets/main-167e11ddb3cf37684b12f71844e6f57af3cc3c85c3c0db40eabf910fff2a2e09.css">
</head>

  <body>
    <header>
  <noscript>
    <div class="container-fluid">
      <div class="card card-block card-danger row">
        <p class="card-text text-sm-center">
          You have Javascript disabled. This website might not work as intended and will look not as good as it can.
          <br />
          Please activate Javascript for this website. We are not loading content from other domains.
        </p>
      </div>
    </div>
  </noscript>
  <nav>
    <div class="navbar navbar-fixed-top navbar-dark bg-inverse" role="navigation">
      <div class="container">
        <button class="navbar-toggler hidden-md-up pull-right" type="button" data-toggle="collapse" data-target="#CollapsingNavbar">
          <i class="fa fa-bars fa-fw"></i>
        </button>

        <a class="navbar-brand" href="/">JLESC</a>

        <div class="collapse navbar-toggleable-sm" id="CollapsingNavbar">
          <ul class="nav navbar-nav">
            <li class="nav-item dropdown">
  <a class="nav-link dropdown-toggle" data-toggle="dropdown" href="#" role="button" aria-haspopup="true" aria-expanded="false">About</a>
  <div class="dropdown-menu">
    <a href="/about/" class="dropdown-item">JLESC</a>
    <a href="/about/partners/" class="dropdown-item">Partners</a>
    <a href="/about/people/" class="dropdown-item">People</a>
    <a href="/about/grants/" class="dropdown-item">Grants</a>
  </div>
</li>

            <li class="nav-item">
              <a href="/news/" class="nav-link">News</a>
            </li>
            <li class="nav-item">
              <a class="nav-link active" href="/events/">Events</a>
            </li>
            <li class="nav-item dropdown">
  <a class="nav-link dropdown-toggle" data-toggle="dropdown" href="#" role="button" aria-haspopup="true" aria-expanded="false">Research</a>
  <div class="dropdown-menu">
    <a href="/research/collaboration/" class="dropdown-item">On Collaboration</a>
    <a href="/projects/" class="dropdown-item">Projects</a>
    <a href="/software/" class="dropdown-item">Software</a>
    <a href="/research/visits/" class="dropdown-item">Visits</a>
  </div>
</li>

            <li class="nav-item">
              <a class="nav-link" href="/references/">References</a>
            </li>
          </ul>

          <ul class="nav navbar-nav pull-right">
            <li class="nav-item">
              <a class="nav-link"
                 href="https://github.com/JLESC/jlesc.github.io/wiki"
                 target="_blank"
                 title="How to contribute"
                 data-toggle="tooltip"
                 data-placement="left">
                <i class="fa fa-fw fa-book"></i>
                How To
              </a>
            </li>
          </ul>
        </div>
      </div>
    </div>
  </nav>
</header>

    <main>
      <div class="container">
        <div class="post post-event">
  <article class="post-content">
    <div class="row">
      <header class="col-sm-12 post-header">
        <h1 class="pull-left">3rd JLESC Workshop</h1>
      </header>
    </div>
    <div class="row">
      <aside class="col-lg-5 col-sm-12 pull-lg-right collapse in" id="meta-aside">
        <section class="event-meta card">
  <div class="card-header">
    <span class="event-kind">Workshop</span> Profile
    <span class="pull-right">
      <a id="hide-meta-aside" role="button" title="Hide Profile" data-toggle="tooltip">
        <i class="fa fa-fw fa-minus"></i>
      </a>
    </span>
  </div>
  
  <ul class="list-group list-group-flush">
    <li class="list-group-item row">
      <label class="col-sm-4">Homepage</label>
      <div class="col-sm-8">
        
          <a href="http://jlesc.bsc.es/" target="_blank"
             title="Event's homepage"
             data-toggle="tooltip"
             role="button">
        
          <i class="fa fa-fw fa-globe"></i>
        
          </a>
        

        
          <a href="http://web.archive.org/web/20151016223451/http://jlesc.bsc.es/" target="_blank"
             class="btn btn-sm btn-link"
             title="Event's homepage in Archive.org Wayback machine"
             data-toggle="tooltip"
             role="button">
        
          <i class="fa fa-history fa-fw"></i>
        
          </a>
        
      </div>
    </li>
    
      <li class="list-group-item row">
        <label class="col-sm-4">Start Date</label>
        <div class="col-sm-8">
          29 Jun 2015
        </div>
      </li>
      <li class="list-group-item row">
        <label class="col-sm-4">End Date</label>
        <div class="col-sm-8">
          01 Jul 2015
        </div>
      </li>
    
    <li class="list-group-item row">
      <label class="col-sm-4">Location</label>
      <div class="col-sm-8">
        
          <a href="https://google.com/maps/search/Barcelona,+Spain"
             target="_blank" class="btn btn-sm btn-link"
             role="button" title="Search on Google Maps" data-toggle="tooltip">
            <i class="fa fa-fw fa-map-marker"></i></a>
          Barcelona, Spain
        
      </div>
    </li>
    
      <li class="list-group-item row">
        <label class="col-sm-4">Hosting Institute</label>
        <div class="col-sm-8">
          <span class="institute">Centro Nacional de Supercomputación</span>
        </div>
      </li>
    
  </ul>
</section>

      </aside>
      <div class="col-lg-7 col-sm-12 pull-lg-left content" id="main-content">
        





<nav id="subnav" class="clearfix" role="navigation">
  <ul class="nav nav-tabs">
    <li class="nav-item">
      <a class="nav-link"
         href="/events/3rd-jlesc-workshop/">Event's Main Page</a>
    </li>
    
      
    
      
    
      
    
      
        
        
      
    
    
      
    
      
    
      
    
      
    
      
        
        
      
    
      
    
      
    
      
        
        
          <li class="nav-item">
            <a class="nav-link active"
               href="/events/3rd-jlesc-workshop/schedule/">Schedule</a>
          </li>
        
      
    
      
        
        
      
    
    <li id="show-meta-aside-wrapper">
      <a class="invisible" id="show-meta-aside" role="button" title="Show Profile" data-toggle="tooltip">
        <i class="fa fa-fw fa-info"></i>
      </a>
    </li>
  </ul>
</nav>

        
          <h2>Schedule</h2>
        
        <div id="schedule-accordion" role="tablist" aria-multiselectable="false">
  <div class="card">
    <div class="card-header" role="tab" id="day-zero-heading">
      <a data-toggle="collapse" data-parent="#schedule-accordion" href="#day-zero-content" aria-expanded="true" aria-controls="day-zero-content">
        <h4>Sunday, June 28th</h4>
      </a>
    </div>
    <div class="card-block collapse in" role="tabpanel" id="day-zero-content" aria-labelledby="day-zero-heading">
      <dl class="dl-horizontal">
        <dt>20:00</dt>
        <dd>
          <strong>Dinner</strong><br />
          <small>
            <a href="https://www.google.com/maps/dir/Barcelona-Sants,+Barcelona,+Spain/Avinguda+Roma,+2+-+4,+08014+Barcelona,+Spain/@41.3796486,2.1393823,17z/data=!3m1!4b1!4m13!4m12!1m5!1m1!1s0x12a49880d2e88101:0x5991aa84fd5ac015!2m2!1d2.140624!2d41.37952!1m5!1m1!1s0x12a49880f67c104f:0x334d1735a46851bb!2m2!1d2.1424321!2d41.3797348?hl=en-US"
               target="_blank" title="Directions" data-toggle="tooltip">
              <i class="fa fa-fw fa-map-marker"></i>
            </a>
            <a href="http://torrecatalunya.expohotels.com/en/restaurant-panoramic-visual-360-barcelona/" target="_blank">
              Visual restaurant
            </a> (2 min. walking distance)
          </small>
        </dd>
      </dl>
    </div>
  </div>

  <div class="card">
    <div class="card-header" role="tab" id="day-one-heading">
      <a class="pull-left" data-toggle="collapse" data-parent="#schedule-accordion" href="#day-one-content" aria-expanded="true" aria-controls="day-one-content">
        <h4>Workshop Day 1 &mdash; Monday, June 29</h4>
      </a>
    </div>
    <div class="card-block collapse in" role="tabpanel" id="day-one-content" aria-labelledby="day-one-heading">
      <p class="card-text">
        Lunch on your own.
        All sessions take place in the hotel on level E.
      </p>

      <dl class="dl-hirozontal">
        <dt>13:00</dt>
        <dd>
          <strong>Registration</strong><br />
          <span class="small">near rooms 5,6 and 7</span>
        </dd>

        <dt>14:00</dt>
        <dd>
          <strong>Welcome and Introduction</strong><br />
          <small>room 5-6 &mdash; chair: <span class="person given-name">Franck</span> <span class="person sur-name">Cappello</span> (<abbr title="Argonne National Laboratory" class="initialism" data-toggle="tooltip">ANL</abbr>)</small>

          <table class="table table-hover table-sm">
            <tbody>
              <tr>
                <td>14:00</td>
                <td><span class="person given-name">Franck</span> <span class="person sur-name">Cappello</span> (<abbr title="Argonne National Laboratory" class="initialism" data-toggle="tooltip">ANL</abbr>)</td>
                <td>Welcome, Workshop objectives and organization</td>
              </tr>
              <tr>
                <td>14:10</td>
                <td><span class="person given-name">Mateo</span> <span class="person sur-name">Valero</span> (<abbr title="Centro Nacional de Supercomputación" class="initialism" data-toggle="tooltip">BSC</abbr>)</td>
                <td><abbr title="Centro Nacional de Supercomputación" class="initialism" data-toggle="tooltip">BSC</abbr> Novelties and vision of the collaboration</td>
              </tr>
              <tr>
                <td>14:20</td>
                <td><span class="person given-name">Antoine</span> <span class="person sur-name">Petit</span> (<abbr title="Institut national de recherche en informatique et en automatique" class="initialism" data-toggle="tooltip">INRIA</abbr>)</td>
                <td><abbr title="Institut national de recherche en informatique et en automatique" class="initialism" data-toggle="tooltip">INRIA</abbr> Novelties and vision of the collaboration</td>
              </tr>
              <tr>
                <td>14:30</td>
                <td><span class="person given-name">Bill</span> <span class="person sur-name">Kramer</span> (<abbr title="University of Illinois at Urbana-Champaign" class="initialism" data-toggle="tooltip">UIUC</abbr>, <abbr title="National Center for Supercomputing Applications" class="initialism" data-toggle="tooltip">NCSA</abbr>)</td>
                <td><abbr title="University of Illinois at Urbana-Champaign" class="initialism" data-toggle="tooltip">UIUC</abbr> Novelties and vision of the collaboration</td>
              </tr>
              <tr>
                <td>14:40</td>
                <td><span class="person given-name">Marc</span> <span class="person sur-name">Snir</span> (<abbr title="Argonne National Laboratory" class="initialism" data-toggle="tooltip">ANL</abbr>)</td>
                <td><abbr title="Argonne National Laboratory" class="initialism" data-toggle="tooltip">ANL</abbr> Novelties and vision of the collaboration</td>
              </tr>
              <tr>
                <td>14:50</td>
                <td><span class="person given-name">Thomas</span> <span class="person sur-name">Lippert</span> (<abbr title="Jülich Supercomputing Centre" class="initialism" data-toggle="tooltip">JSC</abbr>)</td>
                <td><abbr title="Jülich Supercomputing Centre" class="initialism" data-toggle="tooltip">JSC</abbr> Novelties and vision of the collaboration</td>
              </tr>
              <tr>
                <td>15:00</td>
                <td><span class="person given-name">Akira</span> <span class="person sur-name">Ukawa</span> (<abbr title="RIKEN Advanced Institute for Computational Science" class="initialism" data-toggle="tooltip">RIKEN</abbr>)</td>
                <td><abbr title="RIKEN Advanced Institute for Computational Science" class="initialism" data-toggle="tooltip">RIKEN</abbr> Novelties and vision of the collaboration</td>
              </tr>
              <tr>
                <td>15:10</td>
                <td><span class="person given-name">Marc</span> <span class="person sur-name">Snir</span> (<abbr title="Argonne National Laboratory" class="initialism" data-toggle="tooltip">ANL</abbr>)</td>
                <td>
                  <a class="abstract-btn" role="button"
                     href="#abstract-d1-1501-snir" data-toggle="collapse"
                     aria-expanded="false" aria-controls="abstract-d1-1501-snir">
                    <i class="fa fa-fw fa-paragraph"></i>
                  </a>
                  <span class="tag tag-info">Keynote</span> On the Road to Exascale: The next generation of DOE leadership supercomputers
                  <div class="abstract collapse" id="abstract-d1-1501-snir">
                    We shall discuss in this talk the next generation of DOE supercomputers and the changes application codes will need to consider in order to leverage them effectively. We shall next discuss the expected evolution toward the next (exascale) generation of leadership systems.
                  </div>
                </td>
              </tr>
            </tbody>
          </table>
        </dd>

        <dt>15:50</dt>
        <dd><strong>Break</strong></dd>

        <dt>16:10</dt>
        <dd><strong>Welcome and Introduction (cont.)</strong><br />
          <small>chair: <span class="person given-name">Marc</span> <span class="person sur-name">Snir</span> (<abbr title="Argonne National Laboratory" class="initialism" data-toggle="tooltip">ANL</abbr>)</small>

          <table class="table table-hover table-sm">
            <tbody>
              <tr>
                <td>16:10</td>
                <td><span class="person given-name">Thomas</span> <span class="person sur-name">Lippert</span> (<abbr title="Jülich Supercomputing Centre" class="initialism" data-toggle="tooltip">JSC</abbr>)</td>
                <td>
                  <a class="abstract-btn" role="button"
                     href="#abstract-d1-1610-lippert" data-toggle="collapse"
                     aria-expanded="false" aria-controls="abstract-d1-1610-lippert">
                    <i class="fa fa-fw fa-paragraph"></i>
                  </a>
                  <span class="tag tag-info">Keynote</span> Creating the HPC and Data Analytics Infrastructure for the Human Brain Project
                  <div class="abstract collapse" id="abstract-d1-1610-lippert">
                    HBP, the human brain project, is one of two European flagship projects foreseen to run for 10 years. The HBP aims at creating a open European neuroscience driven infrastructure for simulation and big data aided modelling and research with a credible user program. The goal of the HBP is to progressively understand structure and functionality of the human brain, strongly based on a reverse engineering philosophy. In addition, it aims at advancements in digital computing by means of brain inspired algorithms with the potential to create completely novel analogue computing technology called neuromorphic computing. The HBP simulation and data analytics infrastructure will be based on a federation of supercomputer and data centers contributing to specific requirements of neuroscience in a complementary manner. It will encompass a variety of simulation services and data analytics services ranging from the molecular level towards synaptic and neuronal levels up to cognitive and robotic models. The major challenge is that HBP research will require exascale capabilities for computing, data integration and data analytics. Mastering these challenges requires a huge interdisciplinary software and hardware co-design effort including neuroscientists, physicists, mathematicians, and computer scientists on an international scale. The HBP is a long-term endeavor and thus puts large emphasis on educational and training aspects. The maturity of a service is critical, and it is important to differentiate between an early prototype, the development phase, and the delivery of services, in order to assess capability levels. The services and infrastructures of the HBP will successively include more European partners, in particular PRACE sites and EUDAT data services, and will be made available step by step to the pan-European neuroscience community.
                  </div>
                </td>
              </tr>
              <tr>
                <td>16:50</td>
                <td><span class="person given-name">Akira</span> <span class="person sur-name">Ukawa</span> (<abbr title="RIKEN Advanced Institute for Computational Science" class="initialism" data-toggle="tooltip">RIKEN</abbr>)</td>
                <td>
                  <a class="abstract-btn" role="button"
                     href="#abstract-d1-1650-ukawa" data-toggle="collapse"
                     aria-expanded="false" aria-controls="abstract-d1-1650-ukawa">
                    <i class="fa fa-fw fa-paragraph"></i>
                  </a>
                  <span class="tag tag-info">Keynote</span> AICS View toward Exascale
                  <div class="abstract collapse" id="abstract-d1-1650-ukawa">
                    Since this is the first time that RIKEN AICS participates in the JLESC Workshop as a member institution, we wish to present a perspective on various aspects toward exascale from AICS point of view, and the role we hope to play in JLESC in this context. We start by providing a somewhat detailed view of AICS, its founding vision and history, the organization and people, and the science being done. A brief update on the post K Project, officially named the Flagship 2020 Project, is given with some details on application targets and co-design. We then turn to international collaboration from science domain point of view, taking the case of QCD in particle theory as a case study. HPC talks these days would sound lopsided if no mention are made of big data, so we try to do so in a lopsided way in the wrapup.
                  </div>
                </td>
              </tr>
            </tbody>
          </table>
        </dd>

        <dt>17:30</dt>
        <dd><strong>Adjourn</strong></dd>

        <dt>20:00</dt>
        <dd>
          <strong>Dinner</strong><br />
          <small>near meeting rooms</small>
        </dd>
      </dl>
    </div>
  </div>

  <div class="card">
    <div class="card-header" role="tab" id="day-two-heading">
      <a data-toggle="collapse" data-parent="#schedule-accordion" href="#day-two-content" aria-expanded="true" aria-controls="day-two-content">
        <h4>Workshop Day 2 &mdash; Tuesday, June 30</h4>
      </a>
    </div>
    <div class="card-block collapse in" role="tabpanel" id="day-two-content" aria-labelledby="day-two-heading">

      <dl class="dl-horizontal">
        <dt>8:30</dt>
        <dd>
          <strong>Plenary session: <em>Applications and mini-apps</em></strong><br />
          <small>room 5-6 &mdash; chair: <span class="person given-name">Francisco</span> <span class="person sur-name">Doblas-Reyes</span> (<abbr title="Centro Nacional de Supercomputación" class="initialism" data-toggle="tooltip">BSC</abbr>)</small>

          <table class="table table-hover table-sm">
            <tbody>
              <tr>
                <td>8:30</td>
                <td><span class="person given-name">Rob</span> <span class="person sur-name">Jacob</span> (<abbr title="Argonne National Laboratory" class="initialism" data-toggle="tooltip">ANL</abbr>)</td>
                <td>
                  <a class="abstract-btn" role="button"
                     href="#abstract-d2-0830-jacob" data-toggle="collapse"
                     aria-expanded="false" aria-controls="abstract-d2-0830-jacob">
                    <i class="fa fa-fw fa-paragraph"></i>
                  </a>
                  <span class="tag tag-default">Research</span> Challenges of modeling the climate system at Exascale
                  <div class="abstract collapse" id="abstract-d2-0830-jacob">
                    Exascale systems may require several changes to business-as-usual for climate modeling. Using the vertical dimension of the high-horizontal resolution numerical grids may be necessary to obtain more parallelism. Other uses for an exaflop include using high-resolution sub-models in place of paramaterizations based on the large-scale fields and moving other components, such as the land, to fully 3D representations. Ensembles are a necessary and straightforward use of exascale resources. Because of the increase in communication costs, it may no longer be possible to ignore how each model (atmosphere, ocean, etc.) is decomposed relative to each other. Climate models are a multi-physics mixed PDE-ODE application and memory bandwidth will be continue to be important for performance. Early experiments at high resolution indicate that tracer transport will dominate performance at high-resolution making it possible to experiment with different exascale programming models and languages on a single mini-app. Writing to disc will be more expensive at exascale but some data will always need to be output because a great deal of insight in climate modeling comes from comparing two or more simulations against each other. Bit-for-bit reproducibility is heavily relied on during testing and development but it may be possible to relax that requirement for the long production runs that consume most of the time on today’s petascale systems.
                  </div>
                </td>
              </tr>
              <tr>
                <td>8:55</td>
                <td><span class="person given-name">Hisashi</span> <span class="person sur-name">Yashiro</span> (<abbr title="RIKEN Advanced Institute for Computational Science" class="initialism" data-toggle="tooltip">RIKEN</abbr>)</td>
                <td>
                  <a class="abstract-btn" role="button"
                     href="#abstract-d2-0855-yashiro" data-toggle="collapse"
                     aria-expanded="false" aria-controls="abstract-d2-0855-yashiro">
                    <i class="fa fa-fw fa-paragraph"></i>
                  </a>
                  <span class="tag tag-default">Research</span> Climate modeling towards exascale: the case of NICAM-LETKF
                  <div class="abstract collapse" id="abstract-d2-0855-yashiro">
                    Ensemble-based data assimilation system using Nonhydrostatic ICosahedral Atmosphere Model (NICAM) and Local Ensemble Transform Kalman Filter (LETKF) is one of the proxy application for the development of Japanese post-K computer. This combined application requires data throughput in any layer of HPC system such as memory, network and file I/O. Both an appropriate estimation of the expecting hardware/middleware performance and efforts of application side (including the drastic modification of the current source codes) are essential to achieve a high total throughput in the future exa-scale computing. We will introduce our co-design approach in the Japanese post-K project.
                  </div>
                </td>
              </tr>
              <tr>
                <td>9:20</td>
                <td>Stephane Lanteri</td>
                <td>
                  <a class="abstract-btn" role="button"
                     href="#abstract-d2-0950-lanteri" data-toggle="collapse"
                     aria-expanded="false" aria-controls="abstract-d2-0950-lanteri">
                    <i class="fa fa-fw fa-paragraph"></i>
                  </a>
                  <span class="tag tag-default">Research</span> Development of scalable high order finite element solvers for computational nanophotonics in the context of the C2S@Exa Inria Project Lab
                  <div class="abstract collapse" id="abstract-d2-0950-lanteri">
                    This talk will be concerned with the development of hybrid MIMD/SIMD high order finite element solvers for the simulation of the light/matter interaction on the nanoscale. In the first part of the talk, we will present the context of this study, namely the C2S@Exa (Computer and Computational Sciences at Exascale) Inria Project Lab  which is an initiative that was launched in 2013 for a duration of 4 years. C2S@Exa is a multi-disciplinary initiative for high performance computing in computational sciences. In the second part of the talk, we will discuss about our recent efforts towards the design of high performance numerical methodologies based on high order discontinuous Galerkin methods formulated on unstructured meshes, for the solution of the system of time-domain Maxwell equations coupled to dispersive material models relevant to nanophotonics.
                  </div>
                </td>
              </tr>
              <tr>
                <td>9:45</td>
                <td>Andreas Lintermann</td>
                <td>
                  <a class="abstract-btn" role="button"
                     href="#abstract-d2-0945-lintermann" data-toggle="collapse"
                     aria-expanded="false" aria-controls="abstract-d2-0945-lintermann">
                    <i class="fa fa-fw fa-paragraph"></i>
                  </a>
                  <span class="tag tag-default">Research</span> Recent CFD Research in the SimLab FSE
                  <div class="abstract collapse" id="abstract-d2-0945-lintermann">
                    This talk introduces the SimLab concept of the Jülich Supercomputing Center (JSC) and the Jülich Aachen Research Alliance, High Performance Computing (JARA-HPC) and summarizes recent research activities of the SimLab “Highly Scalable Fluids & Solids Engineering” (SLFSE). In more detail, the results of CFD simulations in the field of human respiration, i.e., for the simulation of the flow in the human nasal cavity and particle depositions in the human lung using a Lattice-Boltzmann and a coupled Lagrange particle solver is presented. Furthermore, recent advances in the simulation of aircraft noise and in the optimization of pharmaceutical and chemical processes are given. Finally, prospective research topics and their challenges, i.e., the efficient simulation of respiratory sleep disorders and shape optimizations of shevrons for noise reduction of aircraft engines are discussed.
                  </div>
                </td>
              </tr>
            </tbody>
          </table>
        </dd>

        <dt>10:10</dt>
        <dd><strong>Break</strong></dd>

        <dt>10:35</dt>
        <dd>
          <strong>Plenary session: <em>Applications and mini-apps</em> (cont.)</strong><br />
          <small>room 5-6 &mdash; chair: Naoya Marumaya</small>

          <table class="table table-hover table-sm">
            <tbody>
              <tr>
                <td>10:35</td>
                <td>Mohamed Wahib</td>
                <td>
                  <a class="abstract-btn" role="button"
                     href="#abstract-d2-1035-wahib" data-toggle="collapse"
                     aria-expanded="false" aria-controls="abstract-d2-1035-wahib">
                    <i class="fa fa-fw fa-paragraph"></i>
                  </a>
                  <span class="tag tag-default">Research</span> Scalable and Automated GPU kernel Transformations in Production Stencil Applications
                  <div class="abstract collapse" id="abstract-d2-1035-wahib">
                    We present a scalable method for exposing and exploiting hidden localities in production GPU stencil applications. Our target is to find the best permutation of kernel fusions that would minimize redundant memory accesses. To achieve this, we first expose the hidden localities by analyzing inter-kernel data dependencies. Next, we use a scalable search heuristic that relies on a lightweight performance model to identify the best candidate kernel fusions. To make kernel fusion a practical choice, we developed an end-to-end method for automated transformation. A CUDA-to-CUDA transformation collectively replaces the user-written kernels by auto-generated kernels optimized for data reuse. Moreover, the automated method allows us to improve the search process by enabling kernel fission and thread block tuning. We demonstrate the practicality and effectiveness of the proposed end-to-end automated method. With minimum intervention from the user, we improved the performance of six production applications with speedups ranging between 1.12x to 1.76x.
                  </div>
                </td>
              </tr>
              <tr>
                <td>11:00</td>
                <td>Mariano Vázquez</td>
                <td>
                  <a class="abstract-btn" role="button"
                     href="#abstract-d2-1100-vazquez" data-toggle="collapse"
                     aria-expanded="false" aria-controls="abstract-d2-1100-vazquez">
                    <i class="fa fa-fw fa-paragraph"></i>
                  </a>
                  <span class="tag tag-default">Research</span> Large-scale Simulations for Biomedical Research at Organ Level
                  <div class="abstract collapse" id="abstract-d2-1100-vazquez">
                    In this seminar we describe HPC-based simulations of biological systems at organ level: target, methods and strategies. Unlike the molecular domain, large-scale simulations at organ level are still far from being usual, especially if multi-scale / multi-physics is involved. The interest is high indeed, as some of the latest Gordon Bell Prices awarded achievements in this domain. In this talk we describe the research lines of the CASE department.
                  </div>
                </td>
              </tr>
              <tr>
                <td>11:25</td>
                <td>Philippe Helluy</td>
                <td>
                  <a class="abstract-btn" role="button"
                     href="#abstract-d2-1125-helluy" data-toggle="collapse"
                     aria-expanded="false" aria-controls="abstract-d2-1125-helluy">
                    <i class="fa fa-fw fa-paragraph"></i>
                  </a>
                  <span class="tag tag-default">Research</span> A generic Discontinuous Galerkin solver based on OpenCL task graph. Application to electromagnetic compatibility.
                  <div class="abstract collapse" id="abstract-d2-1125-helluy">
                    We present how we have implemented a generic nonlinear Discontinuous Galerkin (DG) method in the OpenCL/MPI framework in order to achieve high efficiency. The implementation relies on a splitting of the DG mesh into sub-domains and sub-zones. Different kernels are compiled according to the zones properties. We rely on the OpenCL asynchronous task graph driver in order to overlap OpenCL computations and data transfers. We show real-world industrial electromagnetic applications.
                  </div>
                </td>
              </tr>
            </tbody>
          </table>
        </dd>

        <dt>11:50</dt>
        <dd>
          <strong>Open Microphone</strong>: defining a common objective &mdash; establishing the JLESC benchmarks<br />
          <small>chair: Naoya Marumaya</small>
        </dd>

        <dt>12:40</dt>
        <dd>Lunch</dd>

        <dt>14:00</dt>
        <dd>
          <strong>Parallel Session</strong>
          <ul class="nav nav-tabs parallel-session-tabs" role="tablist">
            <li class="nav-item">
              <a class="nav-link active" data-toggle="tab" href="#day-two-parallel-one-a" role="tab">
                Programming models
              </a>
            </li>
            <li class="nav-item">
              <a class="nav-link" data-toggle="tab" href="#day-two-parallel-two-a" role="tab">
                I/O, Big Data, Visualization
              </a>
            </li>
          </ul>
          <div class="tab-content">
            <div class="tab-pane active" id="day-two-parallel-one-a" role="tabpanel">
              <small>room 7 &mdash; chair: Jean François Mehaut</small>

              <table class="table table-hover table-sm">
                <tbody>
                  <tr>
                    <td>14:00</td>
                    <td>Hitoshi Murai</td>
                    <td>
                      <a class="abstract-btn" role="button"
                         href="#abstract-d2-1400-murai" data-toggle="collapse"
                         aria-expanded="false" aria-controls="abstract-d2-1400-murai">
                        <i class="fa fa-fw fa-paragraph"></i>
                      </a>
                      <span class="tag tag-default">Research</span> Overview and Future Plan of the XcalableMP Parallel Language
                      <div class="abstract collapse" id="abstract-d2-1400-murai">
                        XcalableMP (XMP) is a PGAS language for distributed-memory parallel computers. It supports two models of parallel programming for high performance and productivity: directive-based global-view one and RDMA-based local-view one. In this talk, we show the overview of the XMP language specification and the implementation of our Omni XMP compiler. Furthermore, we explain the progress of designing the next version, XcalableMP 2.0.
                      </div>
                    </td>
                  </tr>
                  <tr>
                    <td>14:25</td>
                    <td>Jesus Labarta</td>
                    <td><span class="tag tag-default">Research</span></td>
                  </tr>
                  <tr>
                    <td>14:50</td>
                    <td>Emmanuel Jeannot</td>
                    <td>
                      <a class="abstract-btn" role="button"
                         href="#abstract-d2-1450-jeannot" data-toggle="collapse"
                         aria-expanded="false" aria-controls="abstract-d2-1450-jeannot">
                        <i class="fa fa-fw fa-paragraph"></i>
                      </a>
                      <span class="tag tag-default">Research</span> Improving parallel I/O with topology-aware aggregators mapping
                      <div class="abstract collapse" id="abstract-d2-1450-jeannot">
                        The standard behavior for parallel I/O with MPI consist in electing some aggregators in a set of processes (Pset) to gather the pieces of data and write them to disk (I/O node). The way the aggregators are elected follow a greedy policy by default. As an intelligent mapping of processes is able to reduce the communication cost between them, a relevant choice of the aggregator can induce some gains in term of congestion, access cost and communication cost.
                      </div>
                    </td>
                  </tr>
                  <tr>
                    <td>15:15</td>
                    <td><span class="person given-name">Pavan</span> <span class="person sur-name">Balaji</span> (<abbr title="Argonne National Laboratory" class="initialism" data-toggle="tooltip">ANL</abbr>)</td>
                    <td>
                      <a class="abstract-btn" role="button"
                         href="#abstract-d2-1515-balaji" data-toggle="collapse"
                         aria-expanded="false" aria-controls="abstract-d2-1515-balaji">
                        <i class="fa fa-fw fa-paragraph"></i>
                      </a>
                      <span class="tag tag-default">Research</span> Buffer-sharing Techniques in Insitu Workflows: Challenges and Pitfalls
                      <div class="abstract collapse" id="abstract-d2-1515-balaji">
                        Workflows are gaining increasing popularity to address the needs of scientific computing users that require multiple applications (or components) to process raw data before it is ready to be used or analyzed by a human.  Traditional workflow models have relied on files as a medium of data exchange between such applications. In the recent past, there has been a flurry of research trying to optimize this model using NVRAM-based data sharing, memory-to-memory communication techniques, and even zero-copy techniques using shared buffers. In this talk, I’ll discuss some of the challenges in such buffer-sharing techniques with respect to its impact on the computational model as well as data representation requirements. Specifically, shared memory buffers do not have the same properties as private buffers with respect to how a compiler views them, or how the operating system assigns physical pages to them. This makes it hard for computations to directly be carried out on such shared memory regions at the same efficiency as those on private memory regions. Furthermore, different computations have different data layout requirements. Thus, even if a process can “hand-off” data through a shared-memory buffer, unless the data is laid out exactly how the next application would need it, the cost of working with a bad data layout is often much higher than the cost of simply reorganizing the data before carrying out the required computation. The talk will likely raise more questions than give answers, but is intended to showcase a problem of interest and seek potential collaborations in our search for a solution.
                      </div>
                    </td>
                  </tr>
                </tbody>
              </table>
            </div>

            <div class="tab-pane" id="day-two-parallel-two-a" role="tabpanel">
              <small>room 5-6 &mdash; chair: Bruno Raffin</small>

              <table class="table table-hover table-sm">
                <tbody>
                  <tr>
                    <td>14:00</td>
                    <td>Robert Sisneros</td>
                    <td>
                      <a class="abstract-btn" role="button"
                         href="#abstract-d2-1400-sisneros" data-toggle="collapse"
                         aria-expanded="false" aria-controls="abstract-d2-1400-sisneros">
                        <i class="fa fa-fw fa-paragraph"></i>
                      </a>
                      <span class="tag tag-default">Research</span> An IDEAL Framework: Recent Work at the Intersection of Big Data and "Big Data"
                      <div class="abstract collapse" id="abstract-d2-1400-sisneros">
                        Many types of “Big Data” are generated in the routine use and maintenance of an HPC resource. In addition to the large, structured scientific data generated by at scale simulations run on supercomputers, the machines themselves generate substantial diagnostic data. The latter is data for which the recent explosion of popular “Big Data” techniques is applicable. In this talk we will first explore the differences in using visualization to analyze these differing types of data. We will then present current work on a web-based visualization framework for “Big Data” that is built on a scientific visualization foundation. The result is IDEAL: the Interactive, Dynamic, Etc. Analytics Library.
                      </div>
                    </td>
                  </tr>
                  <tr>
                    <td>14:25</td>
                    <td>Orcun Yildiz</td>
                    <td>
                      <a class="abstract-btn" role="button"
                         href="#abstract-d2-1425-yildiz" data-toggle="collapse"
                         aria-expanded="false" aria-controls="abstract-d2-1425-yildiz">
                        <i class="fa fa-fw fa-paragraph"></i>
                      </a>
                      <span class="tag tag-default">Research</span> Chronos: Failure-Aware Scheduling in Shared Hadoop Clusters
                      <div class="abstract collapse" id="abstract-d2-1425-yildiz">
                        Hadoop emerged as the de facto state-of-the-art system for MapReduce-based data analytics. The reliability of Hadoop systems depends in part on how well they handle failures. Currently, Hadoop handles machine failures by re-executing all the tasks of the failed machines (i.e., executing recovery tasks). Unfortunately, this elegant solution is entirely entrusted to the core of Hadoop and hidden from Hadoop schedulers. The unawareness of failures therefore may prevent Hadoop schedulers from operating correctly towards meeting their objectives (e.g., fairness, job priority) and can significantly impact the performance of MapReduce applications. This paper presents Chronos, a failure-aware scheduling strategy that enables early yet smart action for fast failure recovery while still operating within a specific scheduler objective. Upon failure detection, rather than waiting an uncertain amount of time to get resources for recovery tasks, Chronos leverages a waste-free preemption technique to carefully allocate these resources. In addition, Chronos considers data locality when scheduling recovery tasks to further improve the performance. We demonstrate the utility of Chronos by combining it with Fifo and Fair schedulers. The experimental results show that Chronos recovers to a correct scheduling behavior within a couple of seconds only and reduces the job completion times by up to 43% compared to state-of-the-art schedulers.
                      </div>
                    </td>
                  </tr>
                  <tr>
                    <td>14:50</td>
                    <td>Ramon Nou</td>
                    <td>
                      <a class="abstract-btn" role="button"
                         href="#abstract-d2-1450-nou" data-toggle="collapse"
                         aria-expanded="false" aria-controls="abstract-d2-1450-nou">
                        <i class="fa fa-fw fa-paragraph"></i>
                      </a>
                      <span class="tag tag-default">Research</span> Performance Impacts with Reliable Parallel File Systems at Exascale Level
                      <div class="abstract collapse" id="abstract-d2-1450-nou">
                        The introduction of Exascale storage into production systems will lead to an increase on the number of storage servers needed by parallel file systems. In this scenario, parallel file system designers should move from the current replication configurations to the more space and energy efficient erasure-coded configurations between storage servers. Unfortunately, the current trends on energy efficiency are directed to creating less powerful clients, but a larger number of them (light-weight Exascale nodes), increasing the frequency of write requests and therefore creating more parity update requests. We investigate RAID-5 and RAID-6 parity-based reliability organizations in Exascale storage systems. We propose two software mechanisms to improve the performance of write requests. The first mechanism reduces the number of operations to update a parity block, improving the performance of writes up to 200%. The second mechanism allows applications to notify when reliability is needed by the data, delaying the parity calculation and improving the performance up to a 300%. Using our proposals, traditional replication schemes can be replaced by reliability models like RAID-5 or RAID-6 without the expected performance loss.
                      </div>
                    </td>
                  </tr>
                  <tr>
                    <td>15:15</td>
                    <td>Wolfgang Frings</td>
                    <td>
                      <a class="abstract-btn" role="button"
                         href="#abstract-d2-1515-frings" data-toggle="collapse"
                         aria-expanded="false" aria-controls="abstract-d2-1515-frings">
                        <i class="fa fa-fw fa-paragraph"></i>
                      </a>
                      <span class="tag tag-default">Research</span> Task-Local Parallel I/O Support for Parallel Performance Analysis Tools with SIONlib
                      <div class="abstract collapse" id="abstract-d2-1515-frings">
                        Parallel performance analysis tools like Scalasca and the performance measurement runtime infrastructure Score-P often need to store event traces in multiple task-local files efficiently to record performance data. For very large numbers of processors, these tools often experience scalability limitations since creating thousands of files simultaneously causes metadata-server contention and large file counts complicate file management. The parallel I/O library SIONlib, which alleviates this issue, has recently been extended to support the special requirements to API and data management of these parallel tools. In this talk we will present briefly the design principles of SIONlib and will cover the additional features of SIONlib for tool support.
                      </div>
                    </td>
                  </tr>
                  <tr>
                </tbody>
              </table>
            </div>
          </div>
        </dd>

        <dt>15:40</dt>
        <dd><strong>Break</strong></dd>

        <dt>16:05</dt>
        <dd>
          <strong>Parallel Session</strong>
          <ul class="nav nav-tabs parallel-session-tabs" role="tablist">
            <li class="nav-item">
              <a class="nav-link active" data-toggle="tab" href="#day-two-parallel-one-b" role="tab">
                Programming models
              </a>
            </li>
            <li class="nav-item">
              <a class="nav-link" data-toggle="tab" href="#day-two-parallel-two-b" role="tab">
                I/O, Big Data, Visualization
              </a>
            </li>
          </ul>
          <div class="tab-content">
            <div class="tab-pane active" id="day-two-parallel-one-b" role="tabpanel">
              <small>room 7 &mdash; chair: Rajeev Thakur</small>

              <table class="table table-hover table-sm">
                <tbody>
                  <tr>
                    <td>16:05</td>
                    <td>Jean François Mehaut</td>
                    <td>
                      <a class="abstract-btn" role="button"
                         href="#abstract-d2-1605-mehaut" data-toggle="collapse"
                         aria-expanded="false" aria-controls="abstract-d2-1605-mehaut">
                        <i class="fa fa-fw fa-paragraph"></i>
                      </a>
                      <span class="tag tag-default">Research</span> CORSE: Compiler Optimizations and Runtime SystEms
                      <div class="abstract collapse" id="abstract-d2-1605-mehaut">
                        In this talk, the main research activities of the Corse INRIA team will be presented. The Corse foundation  is related to the combination of static and dynamic techniques of compilation and runtime systems, with always in mind the goal of addressing high performance and low energy challenges. While compilers and runtime systems obviously share a common goal of improving code performance, they play at different levels. Compilers typically apply hardware specific optimizations (register usage, loop unrolling, pipeline, vectorization) to take the most performance out of the underlying architecture’s performance. Runtime Systems, on their side, optimize resource allocation at a macroscopic level. They typically perform load balancing and map application data over the underlying architecture. In both cases – micro and macro – we need information on the underlying architecture in order to select the best options. We believe the two world can mutually benefit from each other by exchanging information and hints about application behavior and hardware capabilities. For instance, runtime systems have no precise information about the behavior of the tasks they have to schedule. Compilers can typically extract useful details such as computational complexity, data access patterns, etc. Transmitting such information to the runtime system (e.g. by attaching properties to tasks) could greatly improve task scheduling policies for instance, by influencing task allocation to better match  target processing units’ capabilities for instance. The software developments will be based on LLVM, OpenMP, Charm++ to integrate the Corse contributions.
                      </div>
                    </td>
                  </tr>
                  <tr>
                    <td>16:30</td>
                    <td>Wen-mei Hwu</td>
                    <td><span class="tag tag-default">Research</span></td>
                  </tr>
                  <tr>
                    <td>16:55</td>
                    <td>Seo Sangmin</td>
                    <td>
                      <a class="abstract-btn" role="button"
                         href="#abstract-d2-1655-sangmin" data-toggle="collapse"
                         aria-expanded="false" aria-controls="abstract-d2-1655-sangmin">
                        <i class="fa fa-fw fa-paragraph"></i>
                      </a>
                      <span class="tag tag-default">Research</span> Argobots: Lightweight Low-level Threading/Tasking Framework
                      <div class="abstract collapse" id="abstract-d2-1655-sangmin">
                        In this talk, we present a lightweight low-level threading and tasking model, called Argobots, to support the massive parallelism required for applications on exascale systems. Argobots' lightweight operations and controllable executions enable high-level programming models to easily tune the performance by hiding long-latency operations with low-cost context switching or by improving locality with cooperative and deterministic scheduling. Often, complex applications are written in hybrid programming models, e.g., MPI+threads, to better exploit inter- and intra-node parallelism on large-scale clusters. Argobots enhances this combination of programming models by exposing a common runtime with interoperable capabilities, providing a shared space where programming models become complementary. We provide an implementation of Argobots as a user-level library and runtime system. Through the evaluation on manycore architectures and clusters, we show that Argobots incurs very low overhead with scalable performance and it is indeed capable of bridging the gap between different programming models.
                      </div>
                    </td>
                  </tr>
                  <tr>
                    <td>17:20</td>
                    <td>Marta Garcia</td>
                    <td>
                      <a class="abstract-btn" role="button"
                         href="#abstract-d2-1720-garcia" data-toggle="collapse"
                         aria-expanded="false" aria-controls="abstract-d2-1720-garcia">
                        <i class="fa fa-fw fa-paragraph"></i>
                      </a>
                      <span class="tag tag-default">Research</span> DLB: Dynamic Load Balancing Library
                      <div class="abstract collapse" id="abstract-d2-1720-garcia">
                        DLB is a dynamic library intended to increase the performance of hybrid applications by improving the load balance within a computational node. DLB will redistribute the computational resources between different processes running in a shared memory node. The load balance will be done at runtime, allowing to solve imbalances coming from different sources, and transparent to the user. DLB supports different programming models and offers and API that can be used by programming models runtime or by application developers to provide useful information for load balancing purposes.
                      </div>
                    </td>
                  </tr>
                  <tr>
                    <td>17:45</td>
                    <td><small>chair: Rajeev Thakur</small></td>
                    <td>Open Microphone: defining a common objective</td>
                  </tr>
                </tbody>
              </table>
            </div>

            <div class="tab-pane" id="day-two-parallel-two-b" role="tabpanel">
              <small>room 5-6 &mdash; chair: Ramon Nou</small>

              <table class="table table-hover table-sm">
                <tbody>
                  <tr>
                    <td>16:05</td>
                    <td>Jorji Nonaka</td>
                    <td>
                      <a class="abstract-btn" role="button"
                         href="#abstract-d2-1605-nonaka" data-toggle="collapse"
                         aria-expanded="false" aria-controls="abstract-d2-1605-nonaka">
                        <i class="fa fa-fw fa-paragraph"></i>
                      </a>
                      <span class="tag tag-default">Research</span> Large-Scale Parallel Image Composition for In Situ Visualization Framework
                      <div class="abstract collapse" id="abstract-d2-1605-nonaka">
                        In situ visualization and analysis approach has been shown as a promising approach for handling the ever increasing size and complexity of large-scale parallel simulation results. In a massively parallel environment, the sort-last visualization method, which requires parallel image composition at the end, has become the de facto standard. Since the image composition process requires the communication among the entire nodes, its performance can potentially be affected if the number of nodes still continues to increase. We have been investigating a parallel image composition approach for the SURFACE (Scalable and Ubiquitous Rendering Framework for Advanced Computing Environment), a visualization framework for the current and next-generation supercomputer.
                      </div>
                    </td>
                  </tr>
                  <tr>
                    <td>16:55</td>
                    <td>Bruno Raffin</td>
                    <td>
                      <a class="abstract-btn" role="button"
                         href="#abstract-d2-1655-raffin" data-toggle="collapse"
                         aria-expanded="false" aria-controls="abstract-d2-1655-raffin">
                        <i class="fa fa-fw fa-paragraph"></i>
                      </a>
                      <span class="tag tag-default">Research</span> In-Situ Processing with FlowVR for Molecular Dynamics Simulations
                      <div class="abstract collapse" id="abstract-d2-1655-raffin">
                        In this talk we will present the FlowVR framework for designing, deploying and executing in situ processing applications. The FlowVR framework combines a flexible programming environment with a runtime enabling efficient executions. Based on a component model, the scientist designs analytics workflows by first developing processing components that are next assembled in a dataflow graph through a Python script. At runtime the graph is instantiated according to the execution context, the framework taking care of deploying the application on the target architecture and coordinating the analytics workflows with the simulation execution. We will present our work through in-situ processing scenarios developed   for analysing  Gromacs molecular dynamics simulations.
                      </div>
                    </td>
                  </tr>
                  <tr>
                    <td>17:20</td>
                    <td>Kate Keahey</td>
                    <td>
                      <a class="abstract-btn" role="button"
                         href="#abstract-d2-1720-keahey" data-toggle="collapse"
                         aria-expanded="false" aria-controls="abstract-d2-1720-keahey">
                        <i class="fa fa-fw fa-paragraph"></i>
                      </a>
                      <span class="tag tag-default">Research</span> Chameleon: Building a Large-scale Experimental Testbed for Cloud Research
                      <div class="abstract collapse" id="abstract-d2-1720-keahey">
                        Cloud services have become essential to all major 21st century economic activities. The new capabilities they enable gave raise to many open questions, some of the most important and contentious issues being the relationship between cloud computing and high performance computing, the suitability of cloud computing for data-intensive applications, and its position with respect to emergent trends such as Software Defined Networking. A persistent barrier to further understanding of those issues has been the lack of a large-scale and open cloud research platforms. With funding from the National Science Foundation, the Chameleon project is providing such a platform to the research community. The testbed, deployed at the University of Chicago and the Texas Advanced Computing Center, will ultimately consist of ~15,000 cores, 5PB of total disk space, and leverage 100 Gbps connection between the sites and consist of a mix of large-scale homogenous hardware and a smaller investment in heterogeneous components high-memory, large-disk, low-power, GPU, and co-processor units. The majority of the testbed is now deployed and available to Early Users with general availability planned for July this year. This talk will provide a detailed description of the available hardware capabilities as well as the workflow allowing users to develop their own experiments. To support a broad range of experiments, the project allows full user configurability of the software stack, from provisioning of bare metal and network interconnects to delivery of fully functioning cloud environments. This is achieved using an infrastructure developed on top of two open source software components: the Grid’5000 software and the widely-adopted OpenStack system. We will discuss the current state of the system as well as projected future features.
                      </div>
                    </td>
                  </tr>
                  <tr>
                    <td>16:30</td>
                    <td>Matthieu Dreher</td>
                    <td>
                      <a class="abstract-btn" role="button"
                         href="#abstract-d2-1630-dreher" data-toggle="collapse"
                         aria-expanded="false" aria-controls="abstract-d2-1630-dreher">
                        <i class="fa fa-fw fa-paragraph"></i>
                      </a>
                      <span class="tag tag-default">Research</span> Data Model and Data Redistribution for In-Situ Applications with Decaf
                      <div class="abstract collapse" id="abstract-d2-1630-dreher">
                        As we are moving toward Exascale, the gap between computational power and I/O bandwidth is becoming more and more concerning. The In-Situ paradigm is one promising solution to this problem. Data are treated and analyzed in memory as close as possible to the source avoiding the I/O for both the simulation and the analytics. Decaf is one infrastructure under development able to connect in-situ heterogeneous parallel codes such as simulations and analytics. The mantra of Decaf is to augment the usual NxM link between two parallel codes (called a Dataflow) with a staging area between the two codes where the user can treat, transform, filter or buffer data. To perform these operations, the user must describe the data to send through Decaf. The data model is a key feature of Decaf to allow future automatic treatments within a Dataflow. However, the shape of the data from simulations and analytics are very different with particular semantics. Data are often breakdown to simple data types or serialized before transmitting data between codes or for I/O for instance. In this talk, after a brief introduction of Decaf, we present a data model able to describe complex data with enough information to chunk and assemble them automatically. While pushing data inside the data model, the user can add annotations to capture the semantic of the data. The user has also the possibility to redefine the automatic behavior of the data model to chunk/assemble data if the data present some particularities. We then discuss the benefits of this data model with the case of data redistribution which can be performed automatically without supplementary user code.
                      </div>
                    </td>
                  </tr>
                  <tr>
                    <td>17:45</td>
                    <td><small>chair: Gabriel Antoniu</small></td>
                    <td>Open Microphone: defining a common objective</td>
                  </tr>
                </tbody>
              </table>
            </div>
          </div>
        </dd>

        <dt>18:10</dt>
        <dd><strong>Adjourn</strong></dd>

        <dt>19:30</dt>
        <dd>
          <strong>Dinner</strong> (meeting at the hotel lobby)<br />
          <small>
            <a href="https://www.google.com/maps/dir/Barcelona-Sants,+Barcelona,+Spain/Arenal+Restaurant,+Passeig+Mar%C3%ADtim+La+Barceloneta,+s%2Fn,+08003+Barcelona/@41.3815855,2.1305101,13z/data=!3m1!4b1!4m14!4m13!1m5!1m1!1s0x12a49880d2e88101:0x5991aa84fd5ac015!2m2!1d2.140624!2d41.37952!1m5!1m1!1s0x12a4a307e4ef03e9:0x44fd0ee725dcb24b!2m2!1d2.194384!2d41.382065!3e2"
               target="_blank" title="Directions" data-toggle="tooltip">
              <i class="fa fa-fw fa-map-marker"></i>
            </a>
            <a href="http://en.arenalrestaurant.com/" target="_blank">
              Arenal Restaurant
            </a>
          </small>
        </dd>
      </dl>
    </div>
  </div>

  <div class="card">
    <div class="card-header" role="tab" id="day-three-heading">
      <a data-toggle="collapse" data-parent="#schedule-accordion" href="#day-three-content" aria-expanded="true" aria-controls="day-three-content">
        <h4>Workshop Day 3 &mdash; Wednesday, July 1</h4>
      </a>
    </div>
    <div class="card-block collapse in" role="tabpanel" id="day-three-content" aria-labelledby="day-three-heading">

      <dl class="dl-horizontal">
        <dt>8:30</dt>
        <dd>
          <strong>Parallel Session</strong>
          <ul class="nav nav-tabs parallel-session-tabs" role="tablist">
            <li class="nav-item">
              <a class="nav-link active" data-toggle="tab" href="#day-three-parallel-one-a" role="tab">
                Resilience
              </a>
            </li>
            <li class="nav-item">
              <a class="nav-link" data-toggle="tab" href="#day-three-parallel-two-a" role="tab">
                I/O, Big Data, Visualization
              </a>
            </li>
          </ul>
          <div class="tab-content">
            <div class="tab-pane active" id="day-three-parallel-one-a" role="tabpanel">
              <small>room 7 &mdash; chair: Osman Unsal</small>

              <table class="table table-hover table-sm">
                <tbody>
                  <tr>
                    <td>8:30</td>
                    <td>Marc Casas</td>
                    <td>
                      <a class="abstract-btn" role="button"
                         href="#abstract-d3-0830-casas" data-toggle="collapse"
                         aria-expanded="false" aria-controls="abstract-d3-0830-casas">
                        <i class="fa fa-fw fa-paragraph"></i>
                      </a>
                      <span class="tag tag-default">Research</span> Asynchronous algorithms to mitigate faults recoveries and enable approximate computing
                      <div class="abstract collapse" id="abstract-d3-0830-casas">
                        Asynchronous algorithms have shown to be useful to enable different kinds of low-overhead resilience strategies. Additionally, since synchronization points constitute an important performance burden in High Performance Computing workloads, novel ideas are starting to emerge to mitigate such costs by trading performance for accuracy bits that do not contribute significantly in the final algorithm output. The talk will provide some results recently obtained at BSC to illustrate the potential of asynchronous and approximate computations for the future of HPC.
                      </div>
                    </td>
                  </tr>
                  <tr>
                    <td>8:55</td>
                    <td>Hongyang Sun</td>
                    <td>
                      <a class="abstract-btn" role="button"
                         href="#abstract-d3-0855-sun" data-toggle="collapse"
                         aria-expanded="false" aria-controls="abstract-d3-0855-sun">
                        <i class="fa fa-fw fa-paragraph"></i>
                      </a>
                      <span class="tag tag-default">Research</span> Which Verification for Soft Error Detection?
                      <div class="abstract collapse" id="abstract-d3-0855-sun">
                        Many methods are available to detect silent errors in high-performance computing (HPC) applications. Each comes with a given cost and recall (fraction of all errors that are actually detected). The main contribution of this paper is to show which detector(s) to use, and to characterize the optimal computational pattern for the application: how many detectors of each type to use, together with the length of the work segment that precedes each of them. We conduct a comprehensive complexity analysis of this optimization problem, showing NP-completeness and designing an FPTAS (Fully Polynomial-Time Approximation Scheme). On the practical side, we provide a greedy algorithm whose performance is shown to be close to the optimal for a realistic set of evaluation scenarios.
                      </div>
                    </td>
                  </tr>
                  <tr>
                    <td>9:20</td>
                    <td>Leonardo Bautista Gomez</td>
                    <td>
                      <a class="abstract-btn" role="button"
                         href="#abstract-d3-0920-gomez" data-toggle="collapse"
                         aria-expanded="false" aria-controls="abstract-d3-0920-gomez">
                        <i class="fa fa-fw fa-paragraph"></i>
                      </a>
                      <span class="tag tag-default">Research</span> Analytic Based Corruption Detection
                      <div class="abstract collapse" id="abstract-d3-0920-gomez">
                        The reliability of future high performance computing systems is one of the biggest challenges to overcome in order to achieve exascale computing. The number of components of supercomputers is increasing exponentially and the power consumption restrictions limit the amount of error verification mechanisms that can be implemented at the hardware level. The soft error rate is expected to increase dramatically in the coming years, leading to a high probability of silent data corruption. In this talk, we present a thorough overview of multiple analytic based corruption detection mechanisms and the difference between them. This survey includes temporal and spatial detectors and multiple prediction methods. In addition, we explore the impact of correcting suspected corruptions with the predictions made by these detectors .Our results show that it is possible to achieve less than 1% of error on the final results while detecting and correcting suspected corruptions automatically.
                      </div>
                    </td>
                  </tr>
                  <tr>
                    <td>9:45</td>
                    <td>Jon Calhoun</td>
                    <td></td>
                  </tr>
                </tbody>
              </table>
            </div>

            <div class="tab-pane" id="day-three-parallel-two-a" role="tabpanel">
              <small>room 5-6 &mdash; chair: Gabriel Antoniu</small>

              <table class="table table-hover table-sm">
                <tbody>
                  <tr>
                    <td>8:30</td>
                    <td>Francieli Zanon Boito</td>
                    <td>
                      <a class="abstract-btn" role="button"
                         href="#abstract-d3-0830-boito" data-toggle="collapse"
                         aria-expanded="false" aria-controls="abstract-d3-0830-boito">
                        <i class="fa fa-fw fa-paragraph"></i>
                      </a>
                      <span class="tag tag-default">Research</span> I/O Scheduling Algorithm Selection for Parallel File Systems
                      <div class="abstract collapse" id="abstract-d3-0830-boito">
                        High Performance Computing applications rely on Parallel File Systems (PFS) to achieve good performance even when handling large amounts of data. It is usual for HPC systems to provide a shared storage infrastructure for applications. In this situation, when multiple applications are concurrently accessing the shared PFS, their accesses will affect each other in a phenomenon called “interference”, which compromises I/O optimization techniques’ efficacy. In this talk, we focus on I/O scheduling as a tool to alleviate interference’s effects. We have conducted an extensive performance evaluation of five scheduling algorithms at a parallel file system’s data servers. Experiments were executed on different platforms and under different access patterns. Results indicate that schedulers’ results are deeply affected by applications’ access patterns and by the underlying I/O system characteristics – especially by storage devices. Our results have shown that there is no scheduling algorithm able to improve performance for all situations, and the best choice depends on applications’ and storage devices’ characteristics. For these reasons, we will discuss our approach to provide I/O scheduling with adaptivity to applications and devices. We use information about these two aspects to automatically select the best fit in scheduling algorithm to each situation. Our approach has provided better results than using the same algorithm to all situations – without adaptability – due to successfully applying I/O scheduling techniques to improve performance while avoiding situations where it would lead to performance impairment.
                      </div>
                    </td>
                  </tr>
                  <tr>
                    <td>8:55</td>
                    <td>Nicolas Vandenbergen</td>
                    <td>
                      <a class="abstract-btn" role="button"
                         href="#abstract-d3-0855-vandenbergen" data-toggle="collapse"
                         aria-expanded="false" aria-controls="abstract-d3-0855-vandenbergen">
                        <i class="fa fa-fw fa-paragraph"></i>
                      </a>
                      <span class="tag tag-default">Research</span> Experiences with Blue Gene Active Storage
                      <div class="abstract collapse" id="abstract-d3-0855-vandenbergen">
                        We report on experiences with Blue Gene Active Storage (BGAS) on the JUQUEEN system. Use cases of the BGAS system from different fields are presented, with a focus on active-storage-centered applications.
                      </div>
                    </td>
                  </tr>
                  <tr>
                    <td>9:20</td>
                    <td>Florin Isaila</td>
                    <td>
                      <a class="abstract-btn" role="button"
                         href="#abstract-d3-0920-isaila" data-toggle="collapse"
                         aria-expanded="false" aria-controls="abstract-d3-0920-isaila">
                        <i class="fa fa-fw fa-paragraph"></i>
                      </a>
                      Optimizing data staging based on autotuning, coordination and locality exploitation on large scale supercomputers
                      <div class="abstract collapse" id="abstract-d3-0920-isaila">
                        Efficient data handling on high-performance computing platforms is one of the critical obstacle to be overcome for reaching higher levels of scalability. This talk will outline three research activities related to data staging on large scale supercomputers. First, I will present a novel hybrid approach to autotuning parallel I/O based on a combination of analytical and machine learning models. Second, I will discuss a coordination framework that targets to support the global improvement of key aspects of data staging including load-balance, I/O scheduling, and resilience. Finally, I will shortly review some current efforts and results for improving the scalability and performance of the Swift workflow language by leveraging data locality through Hercules, a persistent data store, build around the Memcached distributed memory object caching system.
                      </div>
                    </td>
                  </tr>
                  <tr>
                    <td>9:45</td>
                    <td>Ana Queralt</td>
                    <td>
                      <a class="abstract-btn" role="button"
                         href="#abstract-d3-0945-queralt" data-toggle="collapse"
                         aria-expanded="false" aria-controls="abstract-d3-0945-queralt">
                        <i class="fa fa-fw fa-paragraph"></i>
                      </a>
                      Persistent data as a first-class citizen in parallel programming models
                      <div class="abstract collapse" id="abstract-d3-0945-queralt">
                        DataClay is a storage platform that manages data in the form of objects. It enables the applications on top to deal with distributed persistent objects transparently, in the same way as if they were in memory. In addition, dataClay takes to the limit the concept of moving computation to the data, by never separating the data from the methods that allow to manipulate it. By always keeping data and code together, dataClay makes it easier for programming models such as COMPSs to take advantage of data locality, for instance by means of locality-aware iterators that help to exploit parallelism. The combination of these two technologies provides a powerful solution to access and compute on huge datasets, allowing applications to easily handle objects that are too big to fit in memory or that are distributed among several nodes. In this talk we will address how persistent data can be integrated in the programming model by presenting the integration of dataClay and COMPSs, both from the point of view of an application that manages objects and from the point of view of the runtime.
                    </td>
                  </tr>
                </tbody>
              </table>
            </div>
          </div>
        </dd>

        <dt>10:10</dt>
        <dd><strong>Break</strong></dd>

        <dt>8:30</dt>
        <dd>
          <strong>Parallel Session</strong>
          <ul class="nav nav-tabs parallel-session-tabs" role="tablist">
            <li class="nav-item">
              <a class="nav-link active" data-toggle="tab" href="#day-three-parallel-one-b" role="tab">
                Resilience
              </a>
            </li>
            <li class="nav-item">
              <a class="nav-link" data-toggle="tab" href="#day-three-parallel-two-b" role="tab">
                I/O, Big Data, Visualization
              </a>
            </li>
          </ul>
          <div class="tab-content">
            <div class="tab-pane active" id="day-three-parallel-one-b" role="tabpanel">
              <small>room 7 &mdash; chair: Yves Robert</small>

              <table class="table table-hover table-sm">
                <tbody>
                  <tr>
                    <td>10:35</td>
                    <td>Omer Subasi</td>
                    <td>
                      <a class="abstract-btn" role="button"
                         href="#abstract-d3-1035-subasi" data-toggle="collapse"
                         aria-expanded="false" aria-controls="abstract-d3-1035-subasi">
                        <i class="fa fa-fw fa-paragraph"></i>
                      </a>
                      <span class="tag tag-default">Research</span> Efficient Software-based Fault Tolerance for Memory Errors in the Exascale Era
                      <div class="abstract collapse" id="abstract-d3-1035-subasi">
                        Memory reliability will be one of the major concerns for future HPC and Exascale systems. This concern is mostly attributed to the expected massive increase in memory capacity and the number of memory devices in Exascale systems. Error Correcting Codes (ECC) are the most commonly used techniques for memory systems. However state-of-the art hardware ECCs will not be sufficient in terms of error coverage for future computing systems and stronger hardware ECCs providing more coverage have prohibitive costs in terms of area, power and latency. Software-based solutions are needed to cooperate with hardware. In this work, we propose three runtime-based software mechanisms with diverse fault-tolerance capabilities as well as space/memory costs. This provides the flexibility to tailor fault-tolerance according to the system needs. We show that all three mechanisms incur low performance overhead, on average 3%, and are highly scalable. Somewhat surprisingly, we find that software-based CRC protection is feasible providing correction for up to 32-bit burst (consecutive) and 5-bit arbitrary errors while incurring only 1.7% performance overhead with hardware acceleration. Finally, we provide a recipe for how/when to adapt our mechanisms as well as analyze their reliabilities and error coverages. We find that our design reduces the Chipkill undetected error rate by as high as 10^15 times which is vital considering Exascale error rates.
                      </div>
                    </td>
                  </tr>
                  <tr>
                    <td>11:00</td>
                    <td>Suraj Prabhakaran</td>
                    <td>
                      <a class="abstract-btn" role="button"
                         href="#abstract-d3-1100-prabhakaran" data-toggle="collapse"
                         aria-expanded="false" aria-controls="abstract-d3-1100-prabhakaran">
                        <i class="fa fa-fw fa-paragraph"></i>
                      </a>
                      <span class="tag tag-default">Research</span> Dynamic Node Replacement and Adaptive Scheduling for Fault Tolerance
                      <div class="abstract collapse" id="abstract-d3-1100-prabhakaran">
                        Batch systems traditionally support only static resource management wherein a job’s resource set is unchanged throughout execution. Node failures force the batch systems to restart affected jobs on a fresh allocation (typically from a checkpoint) or replace failed nodes with statically allocated spare nodes. As future systems are expected to have high failure rates, this solution leads to increased job restart overhead, additional long waiting times before job restart and excessive resource wastage. In this talk, we present an extension of the TORQUE/Maui batch system with dynamic resource management facilities that enable instant replacement of failed nodes to affected jobs without requiring a job restart. The proposed batch system supports all job types for scheduling – rigid, moldable, evolving and malleable. We present an algorithm for the combined scheduling of all job types and show how the unique features of various jobs and the scheduling algorithm can expedite node replacements. The overall expected benefit of this approach is a highly resilient cluster environment that ensures timely completion of jobs and maintains high throughput even under frequent node failures.
                      </div>
                    </td>
                  </tr>
                  <tr>
                    <td>11:25</td>
                    <td>Atsushi Hori</td>
                    <td>
                      <a class="abstract-btn" role="button"
                         href="#abstract-d3-1125-hori" data-toggle="collapse"
                         aria-expanded="false" aria-controls="abstract-d3-1125-hori">
                        <i class="fa fa-fw fa-paragraph"></i>
                      </a>
                      <span class="tag tag-default">Research</span> Spare Node Substitution
                      <div class="abstract collapse" id="abstract-d3-1125-hori">
                        In the coming Exa-flops era, fault resilience is believed to be a big issue. One of the recent research trends is user-level fault mitigation where a user program manages failures so that the program can survive from the failures and continue its execution. However, some applications (e.g., stencil applications) can only survive only when the number of nodes involved in its computation is invariant. To cope with this situation, having spare nodes to substitute failed nodes seems to be a good idea. However, at the best of our knowledge, there has been almost no discussion on how and how many spare nodes should be allocated and how the failed nodes should be substituted with spare nodes. In this talk, it will be shown that the possibility of communication performance degradation due to the substitutions and several substitution methods will be presented and discussed.
                      </div>
                    </td>
                  </tr>
                  <tr>
                    <td>11:50</td>
                    <td><small>chair: <span class="person given-name">Franck</span> <span class="person sur-name">Cappello</span> (<abbr title="Argonne National Laboratory" class="initialism" data-toggle="tooltip">ANL</abbr>)</small></td>
                    <td>
                      Open Microphone: defining a common objective<br />
                      <small>Establishing the JLESC Resilience Methodology: failure logs, log analysis tools, SDC injection practices, etc.</small>
                    </td>
                  </tr>
                </tbody>
              </table>
            </div>

            <div class="tab-pane" id="day-three-parallel-two-b" role="tabpanel">
              <small>room 5-6 &mdash; chair: Wolfgan Frings</small>

              <table class="table table-hover table-sm">
                <tbody>
                  <tr>
                    <td>10:35</td>
                    <td>Gabriel Antoniu</td>
                    <td>
                      <a class="abstract-btn" role="button"
                         href="#abstract-d3-1035-antoniu" data-toggle="collapse"
                         aria-expanded="false" aria-controls="abstract-d3-1035-antoniu">
                        <i class="fa fa-fw fa-paragraph"></i>
                      </a>
                      <span class="tag tag-default">Research</span> To Overlap or Not to Overlap: Optimizing Incremental MapReduce Computations for On-Demand Data Upload
                      <div class="abstract collapse" id="abstract-d3-1035-antoniu">
                        Research on cloud-based Big Data analytics has focused so far on optimizing the performance and cost-effectiveness of the computations, while largely neglecting an important aspect: users need to upload massive datasets on clouds for their computations. This paper studies the problem of running MapReduce applications when considering the simultaneous optimization of performance and cost of both the data upload and its corresponding computation taken together. We analyze the feasibility of incremental MapReduce approaches to advance the computation as much as possible during the data upload by using already transferred data to calculate intermediate results. Our key finding shows that overlapping the transfer time with as many incremental computations as possible is not always efficient: a better solution is to wait for enough to fill the computational capacity of the MapReduce cluster. Results show significant performance and cost reduction compared with state-of-the-art solutions that leverage incremental computations in a naive fashion.
                      </div>
                    </td>
                  </tr>
                  <tr>
                    <td>11:00</td>
                    <td>Justin Wozniak</td>
                    <td>
                      <a class="abstract-btn" role="button"
                         href="#abstract-d3-1100-wozniak" data-toggle="collapse"
                         aria-expanded="false" aria-controls="abstract-d3-1100-wozniak">
                        <i class="fa fa-fw fa-paragraph"></i>
                      </a>
                      <span class="tag tag-default">Research</span> Swift Parallel Scripting: Novel Features and Applications
                      <div class="abstract collapse" id="abstract-d3-1100-wozniak">
                        Dataflow languages offer a natural means to express concurrency but are not a natural representation of the architectural features of high-performance, distributed-memory computers. When used as the outermost language in a hierarchical programming model, dataflow is very effective at expressing the overall flow of a computation. In this talk, we will present strategies and techniques used by the Swift dataflow language to obtain good performance task-parallel performance on extremely large computing systems, managing task priorities and locations. We will also present new Swift applications in materialsscience and epidemiology.
                      </div>
                    </td>
                  </tr>
                  <tr>
                    <td>11:25</td>
                    <td>Rosa M Badia</td>
                    <td>
                      <a class="abstract-btn" role="button"
                         href="#abstract-d3-1125-badia" data-toggle="collapse"
                         aria-expanded="false" aria-controls="abstract-d3-1125-badia">
                        <i class="fa fa-fw fa-paragraph"></i>
                      </a>
                      <span class="tag tag-default">Research</span> Programmability in PyCOMPSs
                      <div class="abstract collapse" id="abstract-d3-1125-badia">
                        On of today concerns on application development is programmability. While the concept is well understood, it is difficult to measure. PyCOMPSs programmability is based on sequential programming, and in the preservation of the expresivity and potential of programming languages, only using few additions (annotations and small API). This talk will present several examples of PyCOMPSs programming compared with other programming models, such as Apache Spark, and also examples of porting libraries and codes to this programming model.
                      </div>
                    </td>
                  </tr>
                  <tr>
                    <td>11:50</td>
                    <td><small>chair: Rosa M Badia</small></td>
                    <td>Open Microphone: defining a common objective</td>
                  </tr>
                </tbody>
              </table>
            </div>
          </div>
        </dd>

        <dt>12:15</dt>
        <dd><strong>Lunch</strong></dd>

        <dt>13:30</dt>
        <dd>
          <strong>Plenary session: <em>Applications and mini-apps</em></strong><br />
          <small>room 5-6 &mdash; chair: Naoya Marumaya</small>

          <table class="table table-hover table-sm">
            <tbody>
              <tr>
                <td>13:30</td>
                <td>Dr. Gabrielle	Allen</td>
                <td>
                  <a class="abstract-btn" role="button"
                     href="#abstract-d3-1330-allen" data-toggle="collapse"
                     aria-expanded="false" aria-controls="abstract-d3-1330-allen">
                    <i class="fa fa-fw fa-paragraph"></i>
                  </a>
                  <span class="tag tag-default">Research</span> The Einstein Toolkit: A Community Computational Infrastructure for Relativistic Astrophysics
                  <div class="abstract collapse" id="abstract-d3-1330-allen">
                    The Einstein Toolkit is a community-driven software platform of core computational tools to advance and support research in relativistic astrophysics and gravitational physics, with goals to broaden and support the numerical relativity and computational astrophysics communities; to facilitate interdisciplinary collaborations, and to leverage and drive advances in high-end computing cyber infrastructure. Currently, the Einstein Toolkit involves over 100 registered users from over 50 different research groups world-wide and the premier tools enabling discoveries in strong and dynamical space-time phenomena. Furthermore, the toolkit is one of the platforms of choice to take advantage of the most powerful computational hardware available to study astrophysical systems endowed with complex multi-scale/multi-physics properties and governed by Einstein’s equations of General Relativity. This talk gives a brief overview of the current status of the Einstein Toolkit and provides some future directions.
                  </div>
                </td>
              </tr>
            </tbody>
          </table>
        </dd>

        <dt>14:00</dt>
        <dd>
          <strong>Parallel Session</strong>
          <ul class="nav nav-tabs parallel-session-tabs" role="tablist">
            <li class="nav-item">
              <a class="nav-link active" data-toggle="tab" href="#day-three-parallel-three-a" role="tab">
                Performance and tools
              </a>
            </li>
            <li class="nav-item">
              <a class="nav-link" data-toggle="tab" href="#day-three-parallel-four-a" role="tab">
                Numerical Methods/Algorithms
              </a>
            </li>
          </ul>
          <div class="tab-content">
            <div class="tab-pane active" id="day-three-parallel-three-a" role="tabpanel">
              <small>room 5-6 &mdash; chair: Judit Gimenez</small>

              <table class="table table-hover table-sm">
                <tbody>
                  <tr>
                    <td>14:00</td>
                    <td>Miguel Castrilló</td>
                    <td>
                      <a class="abstract-btn" role="button"
                         href="#abstract-d3-1400-castrillo" data-toggle="collapse"
                         aria-expanded="false" aria-controls="abstract-d3-1400-castrillo">
                        <i class="fa fa-fw fa-paragraph"></i>
                      </a>
                      <span class="tag tag-default">Research</span> BSC tools to study the computational efficiency of EC-Earth components
                      <div class="abstract collapse" id="abstract-d3-1400-castrillo">
                        In this talk, we will present real and practical applications of the BSC performance tools use to analyse and understand the performance and also develop optimisations for the EC-EARTH climate model. This model is developed by the EC-Earth consortium, and is widely used at BSC for climate prediction forecasts. The EC-Earth component models are IFS for the atmosphere, NEMO for the ocean, and LIM for the sea-ice, coupled through OASIS. A coupled model composed by different components and running in different configurations and resolutions is a challenge for computer scientists to identify the parts of the code that should be improved to increase application performance. Methodology and examples of improvements done will be presented and discussed.
                      </div>
                    </td>
                  </tr>
                  <tr>
                    <td>14:25</td>
                    <td>Arnaud Legrand</td>
                    <td>
                      <a class="abstract-btn" role="button"
                         href="#abstract-d3-1425-legrand" data-toggle="collapse"
                         aria-expanded="false" aria-controls="abstract-d3-1425-legrand">
                        <i class="fa fa-fw fa-paragraph"></i>
                      </a>
                      <span class="tag tag-default">Research</span> Fast and Accurate Simulation of Multithreaded Dense and Sparse Linear Algebra Solvers
                      <div class="abstract collapse" id="abstract-d3-1425-legrand">
                        Multi-core architectures comprising several GPUs have become mainstream in the field of High Performance Computing. However, obtaining the maximum performance of such heterogeneous machines is challenging as it requires to carefully offload computations and manage data movements between the different processing units. The most promising and successful approaches so far build on task-based runtimes that abstract the machine and rely on opportunistic scheduling algorithms. As a consequence, the problem gets shifted to choosing the task granularity, task graph structure, and optimizing the scheduling strategies. Trying different combinations of these different alternatives is also itself a challenge. Indeed, getting accurate measurements requires reserving the target system for the whole duration of experiments. Furthermore, observations are limited to the few available systems at hand and may be difficult to generalize. We show how we crafted a coarse-grain hybrid simulation/emulation of StarPU, a dynamic runtime for hybrid architectures, over SimGrid, a versatile simulator for distributed systems. This approach allows to obtain performance predictions of both classical dense and sparse linear algebra kernels accurate within a few percents and in a matter of seconds while keeping track of aspects such as memory consumption that are critical in the case of sparse linear algebra. This allows both runtime and application designers to quickly decide which optimization/scheduler to enable or whether it is worth investing in higher-end GPUs/additional memory or not. Additionally, it allows to conduct robust and extensive scheduling studies in a controlled environment whose characteristics are very close to real platforms while having reproducible behavior.
                      </div>
                    </td>
                  </tr>
                  <tr>
                    <td>14:50</td>
                    <td>Paul F Baumeister</td>
                    <td>
                      <a class="abstract-btn" role="button"
                         href="#abstract-d3-1450-baumeister" data-toggle="collapse"
                         aria-expanded="false" aria-controls="abstract-d3-1450-baumeister">
                        <i class="fa fa-fw fa-paragraph"></i>
                      </a>
                      <span class="tag tag-default">Research</span> OpenPOWER: First Performance Results for Scientific Applications
                      <div class="abstract collapse" id="abstract-d3-1450-baumeister">
                        We will report on first experiences with selected scientific applications on IBM POWER8 servers with NVIDIA K40 GPUs. Applications from different research fields will be considered, which pose different requirements to hardware architecture. Our performance evaluation based on currently available hardware will be analyzed having the future roadmap for these technologies in mind.
                      </div>
                    </td>
                  </tr>
                  <tr>
                    <td>15:15</td>
                    <td><span class="person given-name">Bill</span> <span class="person sur-name">Kramer</span> (<abbr title="University of Illinois at Urbana-Champaign" class="initialism" data-toggle="tooltip">UIUC</abbr>, <abbr title="National Center for Supercomputing Applications" class="initialism" data-toggle="tooltip">NCSA</abbr>)</td>
                    <td>
                      <a class="abstract-btn" role="button"
                         href="#abstract-d3-1515-kramer" data-toggle="collapse"
                         aria-expanded="false" aria-controls="abstract-d3-1515-kramer">
                        <i class="fa fa-fw fa-paragraph"></i>
                      </a>
                      <span class="tag tag-default">Research</span> Understanding Performance On Extreme Scale System Takes Big Data and Extreme Tools
                      <div class="abstract collapse" id="abstract-d3-1515-kramer">
                        This talk highlights the systematic performance tools and data collection methods in place across the Blue Waters system to begin. It will discuss the needs for better data evaluation tools that can handle billions of data points per day. Using this Petascale example, the talk will make some observations and propose some guiding directions for the Extreme Scale systematic performance evaluation. The second half of the presentation will discuss a planned initiative (proposed but awaiting funding) that will expand the Blue Waters Sustained Petascale Performance (SPP) test into a broader NSF wide sustained performance evaluation method.
                      </div>
                    </td>
                  </tr>
                </tbody>
              </table>
            </div>

            <div class="tab-pane" id="day-three-parallel-four-a" role="tabpanel">
              <small>room 7 &mdash; chair: Paul Hovland</small>

              <table class="table table-hover table-sm">
                <tbody>
                  <tr>
                    <td>14:00</td>
                    <td>Vijay Mahadevan</td>
                    <td>
                      <a class="abstract-btn" role="button"
                         href="#abstract-d3-1400-vijay" data-toggle="collapse"
                         aria-expanded="false" aria-controls="abstract-d3-1400-vijay">
                        <i class="fa fa-fw fa-paragraph"></i>
                      </a>
                      <span class="tag tag-default">Research</span> Easing computational workflows through flexible and scalable tools
                      <div class="abstract collapse" id="abstract-d3-1400-vijay">
                        High fidelity computational modeling of complex, coupled physical phenomena occurring in several scientific fields require accurate resolution of intricate geometry features (CGM), generation of good quality unstructured meshes that minimize modeling errors (MeshKit), scalable interfaces to load/manipulate/traverse these meshes in memory (MOAB), ability to leverage efficient nonlinear solvers on current and future architectures (PETSc) and support for checkpointing and in-situ visualization. The application of these libraries in a component-based architecture allows flexible usage for improving scientific productivity in several use-cases in order to tackle the heterogeneous descriptions of physical models and for resolving the stiff nonlinearity in coupled multi-physics (CouPE). The usage of these scalable meshing tools and computational solvers for some coupled-physics demonstration problems in nuclear engineering will be presented and outstanding challenges in numerics, software engineering and co-design will be discussed.
                      </div>
                    </td>
                  </tr>
                  <tr>
                    <td>14:25</td>
                    <td><span class="person given-name">David</span> <span class="person sur-name">Haensel</span> (<abbr title="Jülich Supercomputing Centre" class="initialism" data-toggle="tooltip">JSC</abbr>)</td>
                    <td>
                      <a class="abstract-btn" role="button"
                         href="#abstract-d3-1425-haensel" data-toggle="collapse"
                         aria-expanded="false" aria-controls="abstract-d3-1425-haensel">
                        <i class="fa fa-fw fa-paragraph"></i>
                      </a>
                      <span class="tag tag-default">Research</span> First steps towards an automatic load balancing for the Fast Multipole Method
                      <div class="abstract collapse" id="abstract-d3-1425-haensel">
                        The Fast Multipole Method is a generic toolbox algorithm for many important scientific applications, like molecular dynamics, plasma physics or astrophysics. To reach the maximum performance on different hardware hierarchies a more sophisticated parallelization particularly with regard to the work distribution is required. As a very first step we introduced a high level of abstraction on the algorithm and the communication side. The communication layer features a few communicateable data-types used during the calculation, which are hiding the MPI communication. The top of the algorithm layer features work packages serving as a task manager for every rank. Work packages are constructed out of work units depending on the requirements for the calculation of the final targets. Those work units could be calculation or communication tasks and thus determine if a value is retrieved by calculation or communication. With this structure we will be able to implement a load-balancer making decisions based on different strategies in the future. The major strategy will be a partitioning of the communication graph optimizing the load distribution and minimizing the communication.
                      </div>
                    </td>
                  </tr>
                  <tr>
                    <td>14:50</td>
                    <td>Amanda Bienz</td>
                    <td>
                      <a class="abstract-btn" role="button"
                         href="#abstract-d3-1450-bienz" data-toggle="collapse"
                         aria-expanded="false" aria-controls="abstract-d3-1450-bienz">
                        <i class="fa fa-fw fa-paragraph"></i>
                      </a>
                      <span class="tag tag-default">Research</span> Topology-Aware Performance Modelling
                      <div class="abstract collapse" id="abstract-d3-1450-bienz">
                        Sparse matrix vector multiplication (SpMV) is the main component of many iterative methods. The cost of a SpMV consists of the performance of local computation as well as the cost of communicating values between processors. The computational performance is similar on many high-performance computers, as it depends only on the cost of a floating-point operation. However, the cost of communication varies widely across high performance computers, depending on latency (start up cost of a message), network bandwidth, and topology of a network. Standard performance models, such as the alpha-beta model, often capture the difference between latency and bandwidth. These performance models can be improved by taking into account the various topological parameters, such as the distance each message must travel. Topology-aware methods, such as Abhinav Bhatel’s topology manager, calculate the number of network links that must be traversed for a message to get from one node to another. The standard alpha-beta performance model can be improved with use of these topology-aware methods, allowing the model to capture the additional cost associated with traversing a large number of links and model a minimal cost associated with network contention.
                      </div>
                    </td>
                  </tr>
                  <tr>
                    <td>15:15</td>
                    <td>Luc Giraud</td>
                    <td>
                      <a class="abstract-btn" role="button"
                         href="#abstract-d3-1515-giraud" data-toggle="collapse"
                         aria-expanded="false" aria-controls="abstract-d3-1515-giraud">
                        <i class="fa fa-fw fa-paragraph"></i>
                      </a>
                      <span class="tag tag-default">Research</span> Recent progess on numerical kernels for large scale computing on heterogeneous manycores
                      <div class="abstract collapse" id="abstract-d3-1515-giraud">
                        In this work we will discuss recent progresses on the development, design and implementation of some basic numerical kernels for large scale calculations such as FMM and sparse linear systems solutions including sparse direct and hybrid iterative/direct techniques. We will detail some of their implementations on top of run time systems to address the performance portability across possibly heterogeneous manycore platforms. Finally we will present their parallel performance on a few large scale engineering applications including some from the CS2@Exa initiative.
                      </div>
                    </td>
                  </tr>
                </tbody>
              </table>
            </div>
          </div>
        </dd>

        <dt>15:40</dt>
        <dd><strong>Break</strong></dd>

        <dt>16:05</dt>
        <dd>
          <strong>Parallel Session</strong>
          <ul class="nav nav-tabs parallel-session-tabs" role="tablist">
            <li class="nav-item">
              <a class="nav-link active" data-toggle="tab" href="#day-three-parallel-three-b" role="tab">
                Performance and tools
              </a>
            </li>
            <li class="nav-item">
              <a class="nav-link" data-toggle="tab" href="#day-three-parallel-four-b" role="tab">
                Numerical Methods/Algorithms
              </a>
            </li>
          </ul>
          <div class="tab-content">
            <div class="tab-pane active" id="day-three-parallel-three-b" role="tabpanel">
              <small>room 5-6 &mdash; chair: Bernd Mohr</small>

              <table class="table table-hover table-sm">
                <tbody>
                  <tr>
                    <td>16:05</td>
                    <td>Brian Wylie</td>
                    <td>
                      <a class="abstract-btn" role="button"
                         href="#abstract-d3-1605-wylie" data-toggle="collapse"
                         aria-expanded="false" aria-controls="abstract-d3-1605-wylie">
                        <i class="fa fa-fw fa-paragraph"></i>
                      </a>
                      <span class="tag tag-default">Research</span> VI-HPS and Scalasca
                      <div class="abstract collapse" id="abstract-d3-1605-wylie">
                        The Virtual Institute – High Productivity Supercomputing (VI-HPS) combines the expertise of twelve partner institutions (including JLESC members JSC and BSC) in development and application of tools for HPC program development, analysis and optimisation. VI-HPS provides training in the application of these tools to PRACE Advanced Training Centres in Europe and at the invitation of other organizations around the world, particularly in the form of VI-HPS Tuning Workshops where application developers bring along and assisted to apply the tools to their own codes. JSC contributes the open-source Scalasca toolset for scalable performance analysis of large-scale applications, along with the associated Score-P instrumentation and measurement infrastructure and CUBE analysis report utilities that are also used by other tools (including Periscope, TAU and Vampir).
                      </div>
                    </td>
                  </tr>
                  <tr>
                    <td>16:30</td>
                    <td>Brice Videau</td>
                    <td>
                      <a class="abstract-btn" role="button"
                         href="#abstract-d3-1630-videau" data-toggle="collapse"
                         aria-expanded="false" aria-controls="abstract-d3-1630-videau">
                        <i class="fa fa-fw fa-paragraph"></i>
                      </a>
                      <span class="tag tag-default">Research</span> BOAST: Performance Portability Using Meta-Programming and Auto-Tuning
                      <div class="abstract collapse" id="abstract-d3-1630-videau">
                        Porting and tuning HPC applications to new platforms is of paramount importance but tedious and costly in terms of human resources. Unfortunately those efforts are often lost when migrating to new architectures as optimization are not generally applicable. In the Mont-Blanc European project, in collaboration with BSC, we tackle this problem from several angles. One of them is by using task base runtime (OmpSs) to get adaptive scientific applications. Another one is by promoting scientific application auto-tuning. While computing libraries might be auto-tuned, usually HPC applications are hand-tuned. In the fast paced world of HPC nowadays, we believe that HPC applications kernels should be auto-tuned instead. Unfortunately, the investment to setup a dedicated auto-tuning framework is usually too expensive for a single application. Source to source transformations or compiler based solutions exist but sometimes prove too restrictive to cover all use-cases. We thus propose BOAST a meta-programming framework aiming at generating parametrized source code. The aim is for the programmer to be able to orthogonally express optimizations on a computing kernel, enabling a thorough search of the optimization space. This also allows a lot of code factorization and thus code base reduction. We will demonstrate the use of BOAST on a classical Laplace kernel. Demonstrating how our embedded DSL allowed the description of non trivial optimizations. We will also show how the BOAST framework enabled performance and non regression tests to be performed on the generated code versions, resulting in proven and efficient computing kernels on several architectures.
                      </div>
                    </td>
                  </tr>
                  <tr>
                    <td>16:55</td>
                    <td>Harald Servat Gelabert</td>
                    <td>
                      <a class="abstract-btn" role="button"
                         href="#abstract-d3-1655-gelabert" data-toggle="collapse"
                         aria-expanded="false" aria-controls="abstract-d3-1655-gelabert">
                        <i class="fa fa-fw fa-paragraph"></i>
                      </a>
                      <span class="tag tag-default">Research</span> Study the use of the Folding hardware-based profiler to assist on data distribution for heterogeneous memory systems in HPC
                      <div class="abstract collapse" id="abstract-d3-1655-gelabert">
                        Argonne’s research in data distribution and partitioning for heterogeneous memory compute nodes currently relies on a simulator-based data-oriented profiler as a first stage. The current profiling stage is time-consuming. We are interested in evaluating the possibility of adapting and using the profiling tool “Folding” from BSC for this purpose. Since it is based on hardware counters, it seems clear that the profiling time will be greatly reduced. Given the lossy nature of profilers based on hardware counters, however, it will be interesting to determine if this solution provides sufficient resolution for the subsequent stage to generate a well-optimized data distribution. In this talk we will present the project and its current status.
                      </div>
                    </td>
                  </tr>
                  <tr>
                    <td>17:20</td>
                    <td><small>chair: Bernd Mohr</small></td>
                    <td>Open Microphone: defining a common objective</td>
                  </tr>
                </tbody>
              </table>
            </div>

            <div class="tab-pane" id="day-three-parallel-four-b" role="tabpanel">
              <small>room 7 &mdash; chair: Bill Gropp</small>

              <table class="table table-hover table-sm">
                <tbody>
                  <tr>
                    <td>16:05</td>
                    <td><span class="person given-name">Robert</span> <span class="person sur-name">Speck</span> (<abbr title="Jülich Supercomputing Centre" class="initialism" data-toggle="tooltip">JSC</abbr>)</td>
                    <td>
                      <a class="abstract-btn" role="button"
                         href="#abstract-d3-1605-speck" data-toggle="collapse"
                         aria-expanded="false" aria-controls="abstract-d3-1605-speck">
                        <i class="fa fa-fw fa-paragraph"></i>
                      </a>
                      <span class="tag tag-default">Research</span> Parallel-in-Time Integration with PFASST
                      <div class="abstract collapse" id="abstract-d3-1605-speck">
                        The challenges arising from the extreme levels of parallelism required by todays and future HPC systems mandates the mathematical development of new numerical methods featuring a maximum degree of concurrency. Iterative time integration methods that can provide parallelism along the temporal axis have become increasingly popular over the last years. The recently developed “parallel full approximation scheme in space and time” (PFASST) can integrate multiple time-steps simultaneously in a multigrid-fashioned way. Based on multilevel spectral deferred corrections (MLSDC), PFASST is able to apply multiple coarsening strategies in space and time. Here, careful balancing between aggressive coarsening and fast convergence is necessary. In this talk, we will discuss various, application-tailored coarsening strategies and show recent results on successful combinations of space-parallel solvers with PFASST. We will highlight extreme-scale benchmarks with a multigrid solver on up to 448K cores of the IBM Blue Gene/Q installation JUQUEEN and describe ongoing work on developing a space-time parallel tree code for plasma physics applications using a high-order Boris integrator.
                      </div>
                    </td>
                  </tr>
                  <tr>
                    <td>16:30</td>
                    <td>Andre Schleife</td>
                    <td>
                      <a class="abstract-btn" role="button"
                         href="#abstract-d3-1630-schleife" data-toggle="collapse"
                         aria-expanded="false" aria-controls="abstract-d3-1630-schleife">
                        <i class="fa fa-fw fa-paragraph"></i>
                      </a>
                      <span class="tag tag-default">Research</span> Numerical integrators for a plane-wave implementation of real-time time-dependent density functional theory
                      <div class="abstract collapse" id="abstract-d3-1630-schleife">
                        The adiabatic Born-Oppenheimer approximation is prevalent in electronic-structure simulations and molecular dynamics studies, since it signicantly reduces computational cost, however, within this approximation ultrafast electron dynamics is inaccessible. Achieving a computationally aordable, accurate description of real-time electron dynamics through time-dependent quantum-mechanical theory arguably is one of the greatest challenges in computational materials physics and chemistry today. Several groups are currently exploring real-time time-dependent density functional theory as a possible route and we recently implemented this technique into the highly parallel Qbox/Qb@ll codes. The numerical integration of the time-dependent Kohn-Sham equations is highly non-trivial: Using a plane-wave basis set leads to large Hamiltonians which constrains what integrators can be used without losing computational eciency. Here, we studied various integrators for propagating the single-particle wave functions explicitly in time, while achieving high parallel scalability of the plane-wave pseudopotential implementation. We compare a fourth order Runge-Kutta scheme that we found to be conditionally stable and accurate to an enforced time reversal symmetry algorithm. Both are well-suited for highly parallelized supercomputers as proven by excellent performance on a large number of nodes on BlueGene based \Sequoia” at LLNL and Cray XE6 based \Blue Waters” at NCSA. This allows us to apply our scheme to materials science simulations involving hundreds of atoms and thousands of electrons.
                      </div>
                    </td>
                  </tr>
                  <tr>
                    <td>16:55</td>
                    <td>Guillaume Aupy</td>
                    <td>
                      <a class="abstract-btn" role="button"
                         href="#abstract-d3-1655-aupy" data-toggle="collapse"
                         aria-expanded="false" aria-controls="abstract-d3-1655-aupy">
                        <i class="fa fa-fw fa-paragraph"></i>
                      </a>
                      <span class="tag tag-default">Research</span> Optimal Multistage Algorithm for Adjoint Computation
                      <div class="abstract collapse" id="abstract-d3-1655-aupy">
                        We reexamine the work of Stumm and Walther on multistage algorithms for adjoint computation. We provide an optimal algorithm for this problem when there are two levels of checkpoints, in memory and on disk. Previously, optimal algorithms for adjoint computations were known only for a single level of checkpoints with no writing and reading costs; a well-known example is the binomial checkpointing algorithm of Griewank and Walther. Stumm and Walther extended that binomial checkpointing algorithm to the case of two levels of checkpoints, but they did not provide any optimality results. We bridge the gap by designing the first optimal algorithm in this context. We experimentally compare our optimal algorithm with that of Stumm and Walther to assess the difference in performance.
                      </div>
                    </td>
                  </tr>
                  <tr>
                    <td>17:20</td>
                    <td><small>chair: Bill Gropp</small></td>
                    <td>Open Microphone: defining a common objective</td>
                  </tr>
                </tbody>
              </table>
            </div>
          </div>
        </dd>

        <dt>17:45</dt>
        <dd>
          <strong>Closing &mdash; Reviewing defined common objectives</strong><br />
          <small>room 5-6 &mdash; <span class="person given-name">Franck</span> <span class="person sur-name">Cappello</span> (<abbr title="Argonne National Laboratory" class="initialism" data-toggle="tooltip">ANL</abbr>)</small>
        </dd>

        <dt>18:10</dt>
        <dd><strong>Adjourn</strong></dd>

        <dt>19:00</dt>
        <dd>
          <strong>Dinner</strong><br />
          <small>near meeting rooms</small>
        </dd>
      </dl>
    </div>
  </div>
</div>

<div class="row lightbox-container"><a href="#" class="lightbox-link" data-featherlight="/assets/3rd-jlesc-session-ed9305d12c4e41cb14f4f8323be8a1fe84d2d3c3e12413516b5edccca3d5feaf.jpg"><img src="/assets/3rd-jlesc-session-c284939d19f452d8421f63fac890f2f7cb96e29744951b0ab75c9f51bfb1c67f.jpg" class="img-fluid" alt="Participants of the 3rd JLESC Workshop in a session"></a></div>

      </div>
    </div>
  </article>
  
</div>

      </div>
    </main>
    <footer id="site-footer">
  <div class="navbar navbar-full navbar-light">
    <div class="container">
      <div class="row small">
        <div class="col-xs-6 col-sm-3">
          <div class="nav navbar-nav">
            <a class="nav-link" href="/about/imprint/">
              <i class="fa fa-copyright fa-fw"></i><span class="hidden-md-down">Imprint</span></a>
          </div>
        </div>
        <div class="col-sm-6 hidden-xs-down">
          <div class="nav navbar-nav text-xs-center">
            <span class="credit">
              Powered by
              <a class="nav-link" href="https://jekyllrb.com">Jekyll</a> and
              <a class="nav-link" href="https://getbootstrap.com">Twitter Bootstrap</a>
            </span>
          </div>
        </div>
        <div class="col-xs-6 col-sm-3">
          <ul class="nav navbar-nav pull-right">
            <li class="nav-item">
              <a class="nav-link" href="https://github.com/JLESC/jlesc.github.io">
                <i class="fa fa-github fa-fw"></i><span class="hidden-md-down">Sources</span></a>
            </li>
            <li class="nav-item">
              <a class="nav-link" href="https://github.com/JLESC/jlesc.github.io/issues">
                <i class="fa fa-bug fa-fw"></i><span class="hidden-md-down">Found a typo?</span></a>
            </li>
          </ul>
        </div>
      </div>
    </div>
  </div>
</footer>

    <!-- Latest compiled and minified JavaScript -->
<script type="text/javascript" src="/assets/main-cc560abaacc504809fd53ed1638bea5e1d3b98220e2bdc3e828b01fb7dd5ca7e.js"></script>

  </body>
</html>
