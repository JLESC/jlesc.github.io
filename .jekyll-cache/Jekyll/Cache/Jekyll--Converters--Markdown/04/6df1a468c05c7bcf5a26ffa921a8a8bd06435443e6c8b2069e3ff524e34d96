I"3<h2 id="research-topic-and-goals">Research topic and goals</h2>

<p>Many applications for example in Density Functional Theory (DFT) used in physics, chemistry, and materials science have to compute eigenvalues and eigenvectors of dense symmetric matrices.
There are several libraries available for that task in high performance computing: ScaLAPACK, EigenExa, ELPA, and Elemental.
On the other hand there exist different supercomputers like K-computer, BlueGene/Q, and Intel clusters with Xeon Phi accelerators or with GPU accelerators.
The eigensolver routines from the different libraries behave differently on the different computer architectures.
In the first step, we evaluate the several aspects of the routines on the existing production and prototype supercomputers available to give the users recommendations which routine to use for their specific task.
At the moment, evaluation is planned with respect to performance, accuracy and reproducibility in hybrid parallel execution. In the next step, we adapt the routines to the architectures.</p>

<p>We expect to provide the JLESC community with an overview of pros and cons of existing software, and a perspective view of better adaptation to present and planned computers and accelerators.</p>

<p>For the goal of the project, we want to publish results of the evaluations so that users can choose the best software for their computer.
In the next step tuning of the existing software should be done and the new results are made available to the community.</p>

<h2 id="results-for-20152016">Results for 2015/2016</h2>

<p>This project was submitted in August 2015, and started from January 2016, officially.
The porting process has already started for a couple of eigensolver libraries and on couple of systems.
We selected three libraries, ELPA, Elemental and EigenExa, and available resources, the K computer, JUQUEEN, and JURECA.</p>

<p>The present status is summarized in the following table.
We confirmed the libraries are ported and perform on the systems;
<em>Hybrid</em> means MPI/OpenMP hybrid parallel mode,
<em>Pure MPI</em> means only MPI parallel mode.</p>

<table class="table table-sm">
  <thead>
    <tr>
      <th style="text-align: left">HW Platform</th>
      <th style="text-align: center">ELPA (2015.11)</th>
      <th style="text-align: center">Elemental (0.85)</th>
      <th style="text-align: center">EigenExa (2.4a)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">K</td>
      <td style="text-align: center">Hybrid <sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup></td>
      <td style="text-align: center">NA <sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup></td>
      <td style="text-align: center">Hybrid</td>
    </tr>
    <tr>
      <td style="text-align: left">JUQUEEN</td>
      <td style="text-align: center">Hybrid</td>
      <td style="text-align: center">Pure MPI <sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup></td>
      <td style="text-align: center">Hybrid</td>
    </tr>
    <tr>
      <td style="text-align: left">JURECA</td>
      <td style="text-align: center">Hybrid</td>
      <td style="text-align: center">Hybrid</td>
      <td style="text-align: center">Hybrid</td>
    </tr>
  </tbody>
</table>

<h2 id="results-for-20162017">Results for 2016/2017</h2>

<ol>
  <li>We have done benchmarks using EigenExa, ELPA, and Elemental on available platforms, K, JUQUEEN and JURECA.</li>
  <li>Some of the combinations are not available due to compilation problems or runtime software. We will also continue the compilation if we get a new version or a new compiler environment.</li>
  <li>On the latter half of 2016/2017, we analyzed the correlation between the number of expected eigenspectrum and computing time cost. It affects to select best dense eigensolver for application users.
    <ul>
      <li>EigenExa very good if the full eigenspectrum is wanted</li>
      <li>Libraries have to be tuned for the architecture</li>
      <li>Best performance with the library tuned for the machine</li>
      <li>On JUQUEEN and JURECA hybrid parallelization with moderate OpenMP part preferred</li>
      <li>On K computer EigenExa best with as little MPI parallelization as necessary</li>
      <li>On K computer ELPA2 best with pure MPI parallelization</li>
      <li>If only 5 percent of eigenspectrum needed even on K computer ELPA2 pure MPI becomes fastest</li>
    </ul>
  </li>
  <li>GPU or accelerator-based eigenvalue solver like MAGMA or ELPA must be examined in 2017/2018. Currently, we have not yet confirmed to build the ELPA GPU extension on JURECA. Also, we would enhance the EigenExa library with an acceleration of GPUs.</li>
</ol>

<h2 id="results-for-20172018">Results for 2017/2018</h2>

<ol>
  <li>We have done benchmarks using EigenExa, ELPA, and Elemental on available platforms, K, JUQUEEN and JURECA and lately on JURECA Booster module.</li>
  <li>As JUQUEEN will be put out of operation by end of March 2018 we only completed old benchmarks. The old compiler does not support C++/11 and the latest Fortran, thus it was not possible to install newer versions of Elemental and ELPA.</li>
  <li>We inspected new versions of ELPA and EigenExa that support KNL processors.</li>
  <li>Support for Elemental is temporarily stopped with version 0.87.7, which is the last stable version and runs on JURECA in pure MPI mode, thus there are only results for JURECA for this version.</li>
  <li>Results are still the same as 2016/17
    <ul>
      <li>EigenExa very good if the full eigenspectrum is wanted</li>
      <li>Libraries have to be tuned for the architecture</li>
      <li>Best performance with the library tuned for the machine</li>
      <li>On JUQUEEN and JURECA hybrid parallelization with moderate OpenMP part preferred</li>
      <li>On K computer EigenExa best with as little MPI parallelization as necessary</li>
      <li>On K computer ELPA2 best with pure MPI parallelization</li>
      <li>If only 5 percent of eigenspectrum needed even on K computer ELPA2 pure MPI becomes fastest</li>
    </ul>
  </li>
  <li>No results for GPU</li>
  <li>First results for KNL on JURECA booster, similar to the results for JURECA</li>
</ol>

<h2 id="results-for-20182019">Results for 2018/2019</h2>

<ol>
  <li>(Cont.) Distributed MPI/OpenMP Parallel
    <ul>
      <li>On four nodes a combination with 16 MPI processes per node and four OpenMP threads per MPI process is as fast as the pure MPI parallelization for ELPA on a KNL system.</li>
      <li>On a single KNL node pure MPI parallelization was better than hybrid.</li>
      <li>Plan update with respect to the machine replacement in very future!!</li>
    </ul>
  </li>
  <li>(Cont.) GPU extensions (standalone/cluster)
    <ul>
      <li>No update up to now</li>
    </ul>
  </li>
  <li>(UPDATE) Manycore extensions (KNL, PEZY SCx, etc.)
    <ul>
      <li>Wilkinson512/Kevd specialized for 2,4,8-wide SIMD intrinsic : 2 versions Wilkinson and Dongarra-Wilkinson</li>
      <li>Parameter survey for NB of Back-transform</li>
    </ul>
  </li>
</ol>

<h2 id="results-for-20192020">Results for 2019/2020</h2>

<ol>
  <li>(Cont.) Distributed MPI/OpenMP Parallel
    <ul>
      <li>Progress will be delivered up to the machine replacement status.</li>
    </ul>
  </li>
  <li>(Cont.) GPU extensions (standalone/cluster)
    <ul>
      <li>We have investigated the performance trend on three GPU-eigensolvers, cuSolver 10.0, MAGMA 2.5.0, and EigenG 2.2d.</li>
      <li>On a Volta V100 board with CUDA 10.0+intel MKL library yielded excellent performance.</li>
      <li>MAGMA (1stege kernel) and EigenG shows totally 700GFLOPS for a full diagonalization operation when N=9700.</li>
      <li>For larger benchmark, when N=30000 and we used EigenG, the elapsed time for the full diagonalization took approximately 100 seconds, and it reached more than one TFLOPS.</li>
    </ul>
  </li>
</ol>

<h2 id="results-for-20202021">Results for 2020/2021</h2>

<p>Basically, the activity in FY2020 was limited due to the Covid19 pandemic circumstance. However, some of our updates can be summarized as follows:</p>

<ol>
  <li>(Cont.) Distributed MPI/OpenMP Parallel
    <ul>
      <li>Implementation of the D&amp;C routine on Fugaku in trial usage in 2020 and early evaluation from the official launch in March 2021.</li>
      <li>Progress and preliminary results will be reported at the next JLESC meeting in 2021.</li>
    </ul>
  </li>
  <li>(Cont.) GPU extensions (standalone/cluster)
    <ul>
      <li>Good acceleration both on JUWELS and JURECA all parts of the computation</li>
      <li>ELPA1 is accelerated very high; however, ELPA2 only shows reasonable acceleration only at a few % of eigenvectors are computed rather than the pure CPU version.</li>
    </ul>
  </li>
</ol>

<h2 id="results-for-20212022">Results for 2021/2022</h2>

<p>Our activity in 2021/2022 was minimal as in 2020/2021. However, after the official launch of the supercomputer Fugaku in March 2021, we have had several benchmarks on Fugaku and established similar outstanding results compared to the previous system (K computer). As follow as the format in previous years, we can summarize.</p>

<ol>
  <li>(Cont.) Distributed MPI/OpenMP Parallel
    <ul>
      <li>The latest EigenExa (version 2.11) was released in December 2021. It performs stably not only on Fugaku but other cluster systems.</li>
      <li>With 4096 compute nodes (about 16PFLOPS), the eigenvalue calculation of a million-dimensional matrix was achieved in less than an hour. It is roughly comparable to the performance of the K-computer.</li>
    </ul>
  </li>
  <li>(Cont.) GPU extensions (standalone/cluster)
    <ul>
      <li>EigenG, GPU version of EigenExa, was confirmed to perform on an A100 card with a matrix size up to 60.000 dimensions.</li>
    </ul>
  </li>
</ol>

<h2 id="software-update-and-descriptions">Software update and descriptions:</h2>

<ul>
  <li>EigenExa : stable version 2.11 (relases on December 2021)</li>
  <li>ELPA : 2018.11.001, 2019.05.001 for devel stage, and 2019.11.001 for next stage.
     Include support for GPU acceleration for ELPA1 and 2.</li>
</ul>

<h2 id="visits-and-meetings">Visits and meetings</h2>

<p>In the 4th JLESC meeting at Bonn, we had a pre-meeting of this project with regard to a research update and planning in 2016 of each member.</p>

<p>Frequent e-mail exchanges between Toshiyuki Imamura and Inge Gutheil.
In the 5th JLESC meeting at Lyon, both met and discussed about this project.
Also, 6th JLESC meeting was hosted by AICS and Inge Gutheil visited AICS.
Toshiyuki Imamura visited the 7th JLESC workshop in UIUC Urbana and the 8th JLESC workshop in Barcelona.
He also visited Juelich on 23th of June after ISC17 to talk about KNL usage.
Organized a minisymposium titled ‘Performance benchmark of standard eigensolver on KNL systems’, at the PMAA2018 conference, 27-29th of June 2019, Zurich, with two presentations from Imamura (“Communication-Avoiding approaches of dense Eigenvalue / SVD problems”) and Gutheil (“Performance benchmark of standard eigensolver on KNL systems”).
Toshiyuki Imamura visited JSC on 24-25th of January 2019 to talk about eigensolvers on GPU environment and so on.</p>

<h2 id="impact-and-publications">Impact and publications</h2>

<!--





-->

<ol class="bibliography"></ol>

<h2 id="future-plans">Future plans</h2>

<!-- In 2016, we divide the first year into 1st and 2nd halves on the management of this project.
For each half, we plan to do as follow.

* 1st 6 months: performance measurements on the available resources

* 2nd 6 months: tuning the existing libraries on the available computers -->

<!--We plan a minisymposium at PMAA 2018 in Zuerich with the title

 <!--*Performance benchmark of standard eigensolver on KNL systems -->

<p>Apply to the computational resource such as
“Large-scale HPC Challenge” Project, Joint Center for Advanced High Performance Computing (JCAHPC).</p>

<p>Benchmarking on GPU acceleration systems.–&gt;</p>

<p>The project finished in 2022.</p>

<h2 id="references">References</h2>

<ol class="bibliography"></ol>
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>On the K computer, ELPA2 performs in a hybrid parallel fashion, but we confirmed that ELPA1 performs in the pure MPI mode. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>Elemental needs C++10, but C++10 functionality is not supported on the current compiler on the K computer. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3" role="doc-endnote">
      <p>Running in the pure MPI mode is confirmed on JUQUEEN and JURECA at the moment. <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>
:ET